#!/usr/bin/env python

from optparse import OptionParser
from readInput import readTable
import os
from subprocess import Popen, PIPE
from pprint import pprint
from glob import glob
import getpass
import stat
import pipeline as pipeline
import sys
import time
import pickle

VER = "1.0"
possible_studies = dict({"ATLLoc":1,"MaskedMM":2,"BaleenMM":8,"AXCPT":2}) 
batch_dir = "/cluster/kuperberg/SemPrMM/MRI/scripts/spm_batches/"
func_dir = "/cluster/kuperberg/SemPrMM/MRI/functionals/"
dicom_dir = "/cluster/kuperberg/SemPrMM/MRI/dicoms/"
meg_dir = "/cluster/kuperberg/SemPrMM/MEG/data/"
meg_scripts = "/cluster/kuperberg/SemPrMM/MEG/scripts/"
mri_scripts = "/cluster/kuperberg/SemPrMM/MRI/scripts/"
running_jobs = [ ]


class ProgrammerError(Exception):
	pass


class UserError(Exception):
	pass


class SystemError(Exception):
	pass	


def archive_to_cluster(data):
	"""
	data: dict with keys:"subject","subject_dicom"
	Copies dicom images from archive dir to dicom dir.
	parallel subjects : YES
	"""
	if data["verbose"]:
		print("archive_to_cluster:")
	data["archive_dir"] = pipeline.find_session([data["subject"],"-x","Kuperberg"])
	if data["archive_dir"] is None:
		print("ALERT:findsession returned nothing for {0}".format(data["subject"]))
		return
	if not os.path.exists(data["dicom_dir"]):
		os.mkdir(data["dicom_dir"])
	pipeline.mirror(data["archive_dir"],data["dicom_dir"],True)


def scan_only(data):
	"""
	data: dict with keys:"subject_dir","subject_dicom"
	Runs unpacksdcmdir in -scanonly mode, which creates data["mri_dir"]/scan.log
	parallel subjects: NO
	"""
	if not os.path.exists(data["mri_dir"]):
		os.mkdir(data["mri_dir"])
	data["scan_path"] = scan_path(data)
	try:
		pipeline.scan_only(data["dicom_dir"],data["mri_dir"],data["scan_path"])
	except OSError:
		print("\nScan_only: Failed\n\nYou need to setup NMRENV before proceeding.\n\n")


def unpack(data):
	"""
	data: dict with keys:"subject_dir","subject_dicom","subject"
	Runs unpacksdcmdir in the full mode.
	Can take a long time, run with care.
	parallel subjects: NO
	"""
	data["cfg_path"] = cfg_path(data)
	if os.access(data["cfg_path"], os.R_OK):
		pipeline.unpack(data["dicom_dir"],data["mri_dir"],data["cfg_path"])
	else:
		print("ALERT: cannot find cfg file for {0}, re-run --scan2cfg".format(data["subject"]))


def cfg_path(data):
	"""
	data: dict with keys:"subject_dir"
	Returns path to cfg file.
	"""
	return os.path.join(data["mri_dir"],"cfg.txt")


def scan_path(data):
	"""
	data: dict with "subject_dir"
	returns path to scan.log file
	"""
	return os.path.join(data["mri_dir"],"scan.log")


def unpack_all(data):
	"""
	data: dict with keys:"subject_dir","subject_dicom","subject"
	Wrapper for all three pieces of unpacking a subject"s dicom dir.
	Runs:
		1) scan_only
		2) scan2cfg
		3) cfg2info
		4) unpack
	parallel subjects: NO
	"""
	scan_only(data)
	scan2cfg(data)
	cfg2info(data)
	unpack(data)	


def scan2cfg(data):
	"""
	data: dict with keys:"subject_dir"
	Converts scan.log to appropriate cfg file and writes to disk.
	"""
	data["scan_path"] = scan_path(data)
	data["cfg_path"] = cfg_path(data)
	dir_map = dict({"MEMPRAGE_4e_p2_1mm_iso":"MPRAGE", "ge_functionals_atlloc":"ATLLoc", 
			"field_mapping":"FieldMap", "ge_functionals_maskmm":"MaskedMM", 
			"ge_functionals_baleen":"BaleenMM", "MEFLASH_8e_1mm_iso_5deg":"MEFLASH",
			"ge_functionals_axcpt":"AXCPT","ge_functionals_axcpt_sc":"AXCPT"})
	scan_list = [["MEMPRAGE_4e_p2_1mm_iso","ok","256","256","1"],
				["ge_functionals_atlloc","ok","160"],["ge_functionals_maskmm","ok","148"],
				["ge_functionals_baleen","ok","130"],["ge_functionals_axcpt","ok","240"],
				["MEFLASH_8e_1mm_iso_5deg","256","256","ok","1"],["field_mapping","ok","1"],
				["ge_functionals_axcpt_sc","ok","140"]]
	good_lines = pipeline.scan_to_cfg(dir_map,dir_map,scan_list,data["scan_path"])
	good_lines.sort(key=lambda x:int(x.split()[0]))
	fieldmaps = [v for v in good_lines if v.split()[1] == "FieldMap"]
	if len(fieldmaps) % 2 == 1:
		print("Odd amount of fieldmaps. --scan2cfg cannot continue. Copy/paste/edit as needed.")
		for line in good_lines:
			print(line)
		return
	good_fieldmaps = []
	func_names = ["MaskedMM","BaleenMM","ATLLoc","AXCPT"]
	for fm in fieldmaps:
		ind = good_lines.index(fm)
		all_prev_func = [r for r in good_lines[0:ind] if r.split()[1] in func_names]
		if len(all_prev_func) < 1:
			print("ALERT: odd field maps. Copy/paste/edit the dump below.")
			for line in good_lines:
				print(line)
			return
		prev_func = all_prev_func[len(all_prev_func)-1]
		run = fm.split()[0]
		func = prev_func.split()[1]
		parts = fm.split()
		(before,dot,filetype) = parts[3].partition(".")
		(nothing,maptext,num) = before.partition("FieldMap")
		if int(num)%2 == 1:
			postfix = "Mag"
		else:
			postfix = "Phase"
		good_fieldmaps.append("{0} {1} {2} FieldMap_{3}_{4}.{5}".format(parts[0],parts[1],parts[2],
			func,postfix,parts[2]))
	not_fieldmaps = [v for v in good_lines if v.split()[1] != "FieldMap"]
	for fm in good_fieldmaps:
		not_fieldmaps.append(fm)
	good_lines = not_fieldmaps
	good_lines.sort(key=lambda x:int(x.split()[0]))
	try:
		write_file_with_list(data["cfg_path"],"\n".join(good_lines),data["verbose"])
	except:
		print("Failure in scan2cfg")
		raise


def info_path(data):
	return os.path.join(data["mri_dir"],"info.txt")


def cfg2info(data):
	cfg_fname = cfg_path(data)
	if not os.path.exists(cfg_fname):
		print("ALERT: cannot find cfg file for {0}".format(data["subject"]))
		raise UserError
	cfg = readTable(cfg_fname)
	info = dict({})
	prepend_zero = lambda x: "00"+x if int(x)<10 else "0"+x
	for study in possible_studies:
		study_dict = dict({})
		runs = [x for x in cfg if x[1] == study]
		if len(runs) > 0:
			study_dict["was_run"] = True
			for run in runs:
				run_num = run[3].split(".")[0].split(study)[1]
				study_dict["Run"+run_num+"XXX"] = prepend_zero(run[0])
			if not (study == "AXCPT" and data["stype"] == "ac"):
				if len(runs) == possible_studies[study]:
					study_dict["complete"] = True
				else:
					study_dict["complete"] = False
			else:
				if len(runs) == 3:
					study_dict["complete"] = True
				else:
					study_dict["complete"] = False
			#find the MPRAGE
			mprage_runs = [x for x in cfg if x[1] == "MPRAGE"]
			study_dict["MPRAGE_runs"] = [prepend_zero(x[0]) for x in mprage_runs]
			#find the Phase and Mag XXX
			for type in ["Phase","Mag"]:
				type_run = [x for x in cfg if study in x[3] and type in x[3]]
				if not len(type_run) == 1:
					print("ALERT: Couldn't find the FieldMap_{0}_{1}.nii in cfg for {2}".format(study,
						type,data["subject"]))
				else:	
					study_dict["FieldMap"+type+"XXX"] = prepend_zero(type_run[0][0])
			info[study] = study_dict
		else:
			study_dict["was_run"] = False
	#write out info.txt as pickled dict
	pipeline.save_data(info,info_path(data),data["verbose"])
	if data["verbose"]:
		pprint("Info:")
		pprint(info)
		
	
def studies_to_setup(data,type):
	info = pipeline.load_data(info_path(data),data["verbose"])
	if data["single_study"]:
		studies = [data["single_study"]]
	else:		
		if "stats" in type:
			studies = [k for k,v in info.iteritems() if os.path.exists(
				touch_file_path(data,k,"preproc","","run"))]
		elif "preproc" in type:
			studies = [k for k,v in info.iteritems() if v["was_run"]]
	return studies


def setup_spm(data,type):
	data["studies_to_setup"] = studies_to_setup(data,type)
	for study in data["studies_to_setup"]:
		job_dir = os.path.join(data["mri_dir"],study,"jobs")
		if not os.path.exists(job_dir):
			os.mkdir(job_dir)
		if "stats" in type:
			stat_dirs = []
			if study != "BaleenMM":
				stat_dirs.append(os.path.join(data["mri_dir"],study,type))
			else:
				for block in ["1","2"]:
					stat_dirs.append(os.path.join(data["mri_dir"],study,type+block))
			for smooth in ["6mm","8mm"]:
				for stat_dir in stat_dirs:
					if not os.path.exists(stat_dir):
						os.mkdir(stat_dir)
					smooth_dir = os.path.join(stat_dir,smooth)
					if not os.path.exists(smooth_dir):
						os.mkdir(smooth_dir)
		if "stats" in type and study == "BaleenMM":
			write_mlab_script(data,study,type,"1")
			write_mlab_script(data,study,type,"2")
		else:
			write_mlab_script(data,study,type,"")
	write_shell_script(data,type)


def shell_script_path(data,type):
	script_dir = os.path.join(data["mri_dir"],"scripts")
	if not os.path.exists(script_dir):
		os.mkdir(script_dir)
	if "recon" in type:
		return os.path.join(script_dir,"recon"+"_"+data["subject"]+".sh")
	elif data["single_study"]:
		return os.path.join(script_dir,data["single_study"].lower()+"_"+type+"_"+data["subject"]+".sh")
	else:
		return os.path.join(script_dir,"all_"+type+"_"+data["subject"]+".sh")


def write_shell_script(data,type):
	shell_script = shell_script_path(data,type)
	commands = []
	commands.append("#!/bin/sh")
	if "recon" in type:
		setup_recon = "recon-all -s " + data["subject"]
		#find all mprages 
		mprages = glob(os.path.join(data["mri_dir"],"MPRAGE","*","MPRAGE*.nii"))
		for mprage in mprages:
			setup_recon = setup_recon + " -i " + mprage
		commands.append(setup_recon + " > {0}".format(os.path.join(data["mri_dir"],"scripts","setup_recon.log")))
		commands.append("nohup recon-all -all -s {0} -mail {1} >& /dev/null &".format(data["subject"],getpass.getuser()))
	else:
		if "stats" in type:
			commands.append("unset DISPLAY")
		mlab_cmd = "nohup matlab7.11 -nosplash -nodesktop"
		if "stats" in type:
			mlab_cmd += " -nodisplay "
		if not "studies_to_setup" in data:
			data["studies_to_setup"] = studies_to_setup(data,type)
		for study in data["studies_to_setup"]:
			if study == "BaleenMM" and "stats" in type:
				job_file = os.path.join(data["mri_dir"],study,"jobs",study.lower()+"_"+
					type+"_block1.m")
				log_file = os.path.join(data["mri_dir"],study,"jobs",study.lower()+"_"+
					type+"_block1.log")
				commands.append("{2} < {0} > {1} {3}".format(job_file,log_file,
					mlab_cmd,data["parallel"]))
				job_file = os.path.join(data["mri_dir"],study,"jobs",study.lower()+"_"+
					type+"_block2.m")
				log_file = os.path.join(data["mri_dir"],study,"jobs",study.lower()+"_"+
					type+"_block2.log")
				commands.append("{2} < {0} > {1} {3}".format(job_file,log_file,
					mlab_cmd,data["parallel"]))
			else:
				job_file = os.path.join(data["mri_dir"],study,"jobs",study.lower()+"_"+type+".m")
				log_file = os.path.join(data["mri_dir"],study,"jobs",study.lower()+"_"+type+".log")
				commands.append("{2} < {0}  > {1} {3}".format(job_file,log_file,
					mlab_cmd,data["parallel"]))
	try:
		write_file_with_list(shell_script,"\n".join(commands),data["verbose"])
	except IOError:
		print(commands)
		raise SystemError("Cannot write shell script. See text above traceback for shell commands.")
	make_file_exec(shell_script)		


def make_file_exec(path):
	os.chmod(path,stat.S_IRUSR|stat.S_IWUSR|stat.S_IXUSR|stat.S_IRGRP|stat.S_IWGRP|stat.S_IXGRP|stat.S_IROTH )
	
	
def specific_batch_path(data,study,type,block):
	if study == "BaleenMM" and "stats" in type:
		return os.path.join(data["mri_dir"],study,"jobs",study.lower()+"_"+
			type+"_block"+block+"_job.m")
	else:
		return os.path.join(data["mri_dir"],study,"jobs",study.lower()+"_"+type+"_job.m")
		

def write_mlab_script(data,study,type,block):
	if "stats_outliers" in type:
		good_type = "stats"
	else:
		good_type = type
	batch_dict = matlab_batch_dict(data,study,type,block)
	if study == "BaleenMM" and "stats" in type:
		if block is None:
			raise ProgrammerError("write_batch_files: no block specified for baleenmm stats")
		batch_o = os.path.join(data["mri_dir"],study,"jobs",study.lower()+"_"+
			type+"_block"+block+".m")
		batch_i = os.path.join(batch_dir,data["stype"],study.lower()+"_"+good_type+"_block"+block+".m")
	else:
		batch_o = os.path.join(data["mri_dir"],study,"jobs",study.lower()+"_"+type+".m")
		batch_i = os.path.join(batch_dir,data["stype"],study.lower()+"_"+good_type+".m")
	pipeline.f2f_replace(batch_i,batch_o,batch_dict,data["verbose"])


def matlab_batch_dict(data,study,type,block):
	replace_dict = build_funcdir_info(data,study,type,block)
	replace_dict["study"] = study.lower()
	replace_dict["type"] = type
	if block:
		replace_dict["block"] = "_block"+block
	else:
		replace_dict["block"] = "<none>"
	if "stats" in type:
		replace_dict["SixSPM"] = os.path.join(data["mri_dir"],study,type,"6mm","SPM.mat")
		replace_dict["EightSPM"] = os.path.join(data["mri_dir"],study,type,"8mm","SPM.mat")
	replace_dict["run_file"] = touch_file_path(data,study,type,block,"run")
	replace_dict["start_file"] = touch_file_path(data,study,type,block,"start")
	replace_dict["email_success"] = "{0} {1} {2}{3} succeeded".format(data["subject"],study,
		type,block)
	replace_dict["email_fail"] =  "{0} {1} {2}{3} failed".format(data["subject"],study,
		type,block)
	return replace_dict


def build_funcdir_info(data,study,type,block):
	"""build_funcdir_info:
	looks for info.txt and uses the dictionary stored that for everything.
	The MPRAGEXXX is filled out here"""
	info = pipeline.load_data(info_path(data),data["verbose"])
	study_dict = info[study];
	study_dict["subject"] = data["subject"]
	study_dict["stat_folder"] = type+block
	if "stats" in type:
		# we need to insert MRs into study_dict
		run_keys = [k for k,v in study_dict.iteritems() if "Run" in k]
		for run_key in run_keys:
			mr_key = run_key.split("XXX")[0]
			run_num = mr_key.split("Run")[1]
			mr_key += "MR"
			if "outliers" in type:
				study_dict[mr_key] = "art_regression_outliers_and_movement_{0}{1}.mat".format(
					study,run_num)
			else:
				study_dict[mr_key] = "rp_{0}{1}.txt".format(study,run_num)
	elif type == "preproc":
		#find mprage xxx (hardcoded to use the first)
		study_dict["MPRAGEXXX"] = study_dict["MPRAGE_runs"][0]
	return study_dict	


def touch_file_path(data,study,type,block,when):
	if block:
		s = os.path.join(data["mri_dir"],study,"jobs",study.lower()+"_"+type+"_block"+block+"_"+when)
	else:
		s = os.path.join(data["mri_dir"],study,"jobs",study.lower()+"_"+type+"_"+when)
	return s


def setup_recon(data):
	write_shell_script(data,"recon")


def run_mri_script(data,type):
	#find all the scripts of this type
	if type == "recon":
		scripts = [shell_script_path(data,type)]
	else:
		if data["single_study"]:
			prefix = data["single_study"].lower()+"_"
		else :
			prefix = "all_"
		scripts = glob(os.path.join(data["mri_dir"],"scripts",prefix+type+"_*"))
	for script in scripts:
		args = ["nohup", script]
		print("\nStarting {0}\n".format(script))
		process = pipeline.run_process(args)
		running_jobs.append(process)


def meg_script(data,type,extra=None):
	"""
	Run a particular meg script as given in type.
	data: the regular dict
	type: name of meg script (e.g. "preProc1","preAnat1",etc)
	extra: list of extra arguments (e.g. for "preProc2", extra would be [{preBlinkTime},
	{postBlinkTime}])
	"""
	script_path = os.path.join(meg_scripts,type)
	if not os.path.exists(script_path):
		raise UserError("meg_script: can not locate {0}".format(type))
	my_args = [script_path,data["subject"]]
	if type == "preProc":
		#look for fif
		fif = glob(os.path.join(data["meg_dir"],"*_raw.fif"))
		#filter our blink and emptyroom
		fif = [x for x in fif if x.find("Blink") == -1 and x.find("emptyroom") == -1]
		N_fif = len(fif)
		print("Fifs:\n{0}".format("\n".join(fif)))
		icamat = glob(os.path.join(data["meg_dir"],"*_raw_ica.mat"))
		N_icamat = len(icamat)
		print("ICA mat:\n{0}".format("\n".join(icamat)))
		blinks = glob(os.path.join(data["meg_dir"],"*.blinks"))
		N_blinks = len(blinks)
		print("Blinks:\n{0}".format("\n".join(blinks)))
		if (N_fif == 0):
			raise UserError("No fif files found for this subject")
		if not (N_fif == N_icamat == N_blinks):
			raise UserError("Unequal amount of fif,ica.mat,and blink files.\nTry Again")
		# DEFAULT EYE BLINK ARGUMENTS FOR PREPROC1
		extra = ["-0.1","0.3"]
	if type == "preAnat":
		setup_bem(data)
		#check that flash was unpacked
		if os.path.exists(os.path.join(data["mri_dir"],"MEFLASH")):
			extra = ["FLASH"]
		else:
			extra = ["WATER"]
	if extra:
		my_args.extend(extra)
	#start the process 
	log_file = "{0}{1}_{2}.log".format(data["meg_dir"],data["subject"],type)
	print("Logging to {0}".format(log_file))
	f = open(log_file,"w")
	process = pipeline.run_process(my_args,output=f,error=f)
	if data["parallel"]:
		running_jobs.append(process)	
	else:
		process.communicate()[0]


def run_ica(data):
	iscript = os.path.join(meg_scripts,"generic_runica.m")
	oscript = os.path.join(data["meg_dir"],data["subject"]+"_ica.m")
	pipeline.f2f_replace(iscript,oscript,dict({"subject":data["subject"]}),data["verbose"])
	log = os.path.join(data["meg_dir"],data["subject"]+"_ica.log")
	my_args = ["matlab7.11", "-nodisplay","-nosplash","-nodesktop", "<", oscript,
		">", log]
	try:
		process = pipeline.run_process(my_args,output=None)
		process.communicate()
	except:
		print("Unexpected error when running ICA")
		raise


def setup_bem(data):
	print("Setup_BEM:{0}".format(data["subject"]))
	#must have env var set
	if os.getenv("SUBJECTS_DIR") is None:
		raise UserError("$SUBJECTS_DIR is not set")
	recon_dir = os.path.join(os.getenv("SUBJECTS_DIR"),data["subject"])
	#if the recon hasn"t run yet quit
	if not os.path.exists(recon_dir):
		raise UserError("Reconstruction folders not setup. Start recon and try again later.")
	bem_dir = os.path.join(recon_dir,"bem")
	if not os.path.exists(bem_dir):
		os.mkdir(os.path.join(recon_dir,"bem"))
	flash_dcm_dir = os.path.join(bem_dir,"flash_dcm")
	if not os.path.exists(flash_dcm_dir):	
		os.mkdir(flash_dcm_dir)
	flash_org_dir = os.path.join(bem_dir,"flash_org")	
	if not os.path.exists(flash_org_dir):	
		os.mkdir(flash_org_dir)
	try:
		scanlog_path = scan_path(data)
		all_lines = list_from_file(scanlog_path,data["verbose"])
		dcm = [line.split()[7] for line in all_lines if "MEFLASH_8e_1mm_iso_5deg" in line and " 8 " in line and " err " not in line]
		if len(dcm) > 1:
			ProgrammerError("More than one MEFLASH5 with 8 frames...check scan.log")
		elif len(dcm) == 0:
			dcm = None;		
		else:
			dcm = dcm[0]
	except IOError:
		raise ProgrammerError("Could not open scan.log file")
	if dcm:
		(sub_dcm,dash,after) = dcm.rpartition("-")
		search_dir = os.path.join(data["dicom_dir"],sub_dcm+"*.dcm")
		flash_dcms = glob(search_dir)
		flash_dcms.sort()
		for flash_dcm in flash_dcms:
			(path,slash,filename) = flash_dcm.rpartition("/")
			try:
				sympath = os.path.join(flash_dcm_dir,filename)
				os.symlink(flash_dcm,sympath)
			except OSError:
				pass
	else:
		print("NO Flash DCM...")


def set_paths(data):
	"""Fairly self-explanatory.  Adds keys to the dictionary for "mri_dir","subject_dir",
	"dicom_dir",and "subject_dicom".  path prefixes are hard coded.
	"""
	data["mri_dir"] = os.path.join(func_dir,data["subject"])+os.sep
	data["dicom_dir"] = os.path.join(dicom_dir,data["subject"])+os.sep
	data["meg_dir"] = os.path.join(meg_dir,data["subject"])+os.sep


def second_level(data,type):
	prefix = os.path.join(func_dir,"SecondLevelStats")
	study_contrasts = dict({"ATLLoc":[["Nonwords","0003"],["WordLists","0002"],["Sentences","0001"],
		["SentencesVWordLists","0004"],["SentencesVNonwords","0006"],["WordListsVNonwords","0008"]],
		"MaskedMM":[["Rel","0001"],["UnRel","0003"],["UnRelVRel","0007"]],
		"BaleenLP":[["Rel","0001"],["UnRel","0002"],["UnRelVRel","0007"]],
		"BaleenHP":[["Rel","0001"],["UnRel","0002"],["UnRelVRel","0008"],
		["UnRelFillvRelFill","0009"]],"AXCPT":[["AYvBY","0009"],["BXvBY","0011"]]})
	study_stats = dict({"ATLLoc":"stats_outliers","MaskedMM":"stats_outliers",
		"BaleenLP":"stats_outliers1","BaleenHP":"stats_outliers2","AXCPT":"stats_outliers"})
	if data["date"]:
		date_dir = data["date"]
	else:
		date_dir = time.strftime("%Y%m%d")
	if type == "setup":
		second_setup(data,prefix,date_dir,study_contrasts,study_stats)
	if type == "run":
		second_run(data,prefix,date_dir,study_contrasts,study_stats)
	if type == "surf":
		second_surf(data,prefix,date_dir,study_contrasts,study_stats)
	if type == "package":
		second_package(data,prefix,date_dir,study_contrasts,study_stats)

		
def second_package(data,prefix,date,study_contrasts,study_stats):
	import shutil
	print("Packaging second level surf analysis....\n")
	package_dir = os.path.join(prefix,"surf_analysis_"+date)
	surf_dir = os.path.join(mri_scripts,"surf_analysis")
	if not os.path.exists(package_dir):
		os.mkdir(package_dir)
	if data["single_study"]:
		studies = [data["single_study"]]
	else:
		studies = ["ATLLoc","BaleenLP","BaleenHP","MaskedMM","AXCPT"]
	#copy index.html
	shutil.copyfile(os.path.join(surf_dir,"index.html"),
		os.path.join(package_dir,"index.html"))
	for study in studies:
		date_dir = os.path.join(prefix,data["stype"],study,date)
		#copy <study>.html
		shutil.copyfile(os.path.join(surf_dir,study+".html"),
			os.path.join(package_dir,study+".html"))
		#within package_dir, make study/
		study_dir = os.path.join(package_dir,study)
		if not os.path.exists(study_dir):
			os.mkdir(study_dir)
		for [contrast,XXXX] in study_contrasts[study]:
			print("{0}:\t{1}...".format(study,contrast))
			#within study_dir, make con/
			con_dir = os.path.join(date_dir,contrast)
			res_con_dir = os.path.join(study_dir,contrast)
			if not os.path.exists(res_con_dir):
				os.mkdir(res_con_dir)
			#data_con_dir points to data contrast directory
			img_con_dir = os.path.join(con_dir,"img")
			#within every data_con_dir is which holds all jpgs
			#we want to copy these jpgs to res_con_dir
			print("Copying images...")
			jpgs = glob(os.path.join(img_con_dir,"*.jpg"))
			for jpg in jpgs:
				shutil.copy(jpg,res_con_dir)
			info_fname = os.path.join(con_dir,"info.txt")
			info = pipeline.load_data(info_fname,data["verbose"])
			#now, make the <con>.html file
			replace_dict = dict({"N":str(info["N"]),"pvalue":str(info["pvalue"]),
				"contrast":info["contrast"],"study":study,"stype":info["stype"],
				"images":"\n".join(["<li>{0}</li>".format(x) for x in info["images"]]),
				"mask":info["mask"]})
			pipeline.f2f_replace(os.path.join(surf_dir,"contrast.html"),
				os.path.join(study_dir,contrast+".html"),replace_dict,data["verbose"])

			
def second_surf(data,prefix,date,study_contrasts,study_stats):
	from scipy import stats
	import shutil
	print("We're going surfing....\n")
	if data["single_study"]:
		studies = [data["single_study"]]
	else:
		studies = ["ATLLoc","BaleenLP","BaleenHP","MaskedMM","AXCPT"]
	for study in studies:
		date_dir = os.path.join(prefix,data["stype"],study,date)
		for [contrast,XXXX] in study_contrasts[study]:
			print("\n{0}:\t{1}...".format(study,contrast))
			con_dir = os.path.join(date_dir,contrast)
			#first, check that --setup_second has been run by looking for con_dir
			if not os.path.exists(con_dir):
				print("ERROR:--setup_second hasn't been run.")
				print("Do this and try again or pass in a specific date by --date=YYYYMMDD")
				break
			#next, check that SPM has been run and that spmT_0001.img exists
			t_img_exists = len(glob(os.path.join(con_dir,"spmT_0001.img"))) == 1
			if not t_img_exists:
				print("ERROR:SPM hasn't been run on {0}:{1}".format(study,contrast))
				break
			#spmT_0001.img exists, let's proceed
			#look for the pickled data
			info_fname = os.path.join(con_dir,"info.txt")
			info = pipeline.load_data(info_fname,data["verbose"])
			script_dict = dict({})
			if not "pvalue" in info:
				info["pvalue"] = data["pvalue"]
			if data["override_p"]:
				info["pvalue"] = data["pvalue"]
			#else, assume pvalue is already in info
			info["tvalue"] = stats.t.isf(info["pvalue"],info["N"])
			print("T-threshold at N={0} and p<{1} is {2}".format(info["N"],info["pvalue"],
				info["tvalue"]))
			script_dict["tvalue"] = str(info["tvalue"])
			script_dict["aparc"] = data["aparc"]
			script_dict["con_dir"] = con_dir
			new_script_path = os.path.join(con_dir,"make_images.sh")
			pipeline.f2f_replace(os.path.join(mri_scripts,"surf_analysis","make_images.sh"),
				new_script_path,script_dict,data["verbose"])
			make_file_exec(new_script_path)
			#copy lh_tiffs and rh_tiffs
			for surf_script in ["lh_tiff","rh_tiff"]:
				tiffs_path = os.path.join(mri_scripts,"surf_analysis",surf_script)
				new_tiffs_path = os.path.join(con_dir,surf_script)
				shutil.copyfile(tiffs_path,new_tiffs_path)
			pipeline.save_data(info,info_fname,data["verbose"])
			if not data["dry"]:
				# all ready to run make_images.sh
				try:
					process = pipeline.run_process([new_script_path],sys.stdout)
					output = process.communicate()[0]
				except KeyboardInterrupt:
					sys.exit("Surf canceled by user...Goodbye")

			
def second_setup(data,prefix,date_dir,study_contrasts,study_stats):
	mlab_cmd = "matlab7.11 -nodisplay -nosplash -nodesktop "
	shell_commands = []
	shell_commands.append("#!/bin/sh")
	shell_commands.append("unset DISPLAY")
	if data["single_study"]:
		studies = [data["single_study"]]
	else:
		studies = ["ATLLoc","BaleenLP","BaleenHP","MaskedMM","AXCPT"]
	for study in studies:
		new_dir = os.path.join(prefix,data['stype'],study,date_dir)
		#make the new dir
		if not os.path.exists(new_dir):
			os.mkdir(new_dir)
		job_dir = os.path.join(new_dir,"jobs")
		if not os.path.exists(job_dir):
			os.mkdir(job_dir)
		print("\n{0}:\t{1} contrasts".format(study,len(study_contrasts[study])))
		for [contrast,XXXX] in study_contrasts[study]:
			replace_dict = dict()
			replace_dict["contrast"] = contrast
			con_dir = os.path.join(new_dir,contrast)
			replace_dict["con_dir"] = con_dir
			if not os.path.exists(con_dir):
				os.mkdir(con_dir)
			if "Baleen" in study:
				real_study = "BaleenMM"
			else:
				real_study = study
			search_term = os.path.join(func_dir,data["stype"]+"*",real_study,study_stats[study],"8mm",
				"con_"+XXXX+".img")
			all_img = glob(search_term)
			#add included subjecdt images
			if data["good_sub"]:
				for sub in data["good_sub"]:
					all_img.extend(glob(os.path.join(func_dir,sub,real_study,study_stats[study],
						"8mm","con_"+XXXX+".img")))
			#remove excluded subject images
			good_img = [x for x in all_img if len([bad for bad in data["bad_sub"] if bad in x]) == 0]
			good_img[:] = ["'"+x+"'" for x in good_img]
			N = len(good_img)
			replace_dict["contrast_images"] = "\n".join(good_img)
			#are we masking?
			match = [x for x in data["mask"] if x[0] == study and x[1] == contrast]
			if match:
				mask = match[0][2]
				print("Using mask at {0}".format(mask))
			else:
				mask = ""
			replace_dict["mask"] = mask
			replace_dict["email_fail"] = "{0} {1} 2nd Level failed".format(study,contrast)
			replace_dict["SPM"] = "{0}/SPM.mat".format(con_dir)
			ibatch = os.path.join(batch_dir,data["stype"],"generic_2nd_batch.m")
			obatch = os.path.join(job_dir,contrast+".m")
			#make batch
			pipeline.f2f_replace(ibatch,obatch,replace_dict,data["verbose"])
			log = os.path.join(job_dir,contrast+".log")
			shell_commands.append("{0} < {1} > {2} {3}".format(mlab_cmd,obatch,log,data["parallel"]))
			shell_commands.append("echo {0}:{1} has finished.".format(study,contrast))
			info_fname = os.path.join(con_dir,"info.txt")
			info = dict({"N":N,"images":good_img,"stype":data["stype"],"contrast":contrast})
			if mask:
				info["mask"] = mask
			else:
				info["mask"] = "No mask used"
			pipeline.save_data(info,info_fname,data["verbose"])
			print("{0} -> {1} subjects".format(contrast,N))	
	shell_path = second_script(data)
	write_file_with_list(shell_path,"\n".join(shell_commands),data["verbose"])
	make_file_exec(shell_path)


def second_run(data,prefix,date_dir,study_contrasts,study_stats):
	script_path = second_script(data)
	print("\nStarting {0}".format(script_path))
	process = pipeline.run_process(script_path,output=sys.stdout)
	process.communicate()


def second_script(data):
	return os.path.join(func_dir,"SecondLevelStats",data["stype"],"all_studies.sh")

		
def subject_type(subjects):
	# assume first is good
	if len(subjects) == 1:
		stype = subjects[0][:2]
	else:
		stype = subjects[0][:2]
		match = len([x for x in subjects[1:] if x[:2] == stype]) == (len(subjects) - 1)
# 		if not match:
# 			print("\nWarning, you seem to be processing subjects of differing types!!!!\n")
# 			response = 'n'
# 			response = input("Do you wish to continue? (y/n) : ")
# 			if response != "y":
# 				raise UserError("Try again with a better subject list")
	return stype


def write_file_with_list(path,lines,verbose=None):
	try:
		with open(path,'w') as f:
			f.writelines(lines)
		if verbose:
			print("Wrote {0}".format(path))
	except IOError:
		raise


def list_from_file(path,verbose=None):
	try: 
		with open(path,'r') as f:
			all_lines = f.read()
			all_lines = all_lines.splitlines()
			if verbose:
				print("Reading from {0}".format(path))
	except IOError:
		print("Cannot open {0}".path)
		raise
	return all_lines


def main():
	parser = OptionParser(usage="%prog [options] subject (more subjects)", version="%prog 1.0")
	parser.add_option("-d","--copy_dicom",dest="copy_dicom", 
		help="Copy from archive to $DICOM_DIR/[subject]",action="store_true",default=False)
	parser.add_option("-u","--unpack_all",dest="unpack_all",action="store_true",default=False,
		help="If specified, unpacking occurs in $FUNCTIONAL_DIR/[subject]")
	parser.add_option("-v","--verbose",dest="verbose",help="Print info",action="store_true",
		default=False)
	parser.add_option("-p","--setup_preproc",dest="setup_preproc",action="store_true",
		default=False,help="Make spm (preproc) batch files")
	parser.add_option("-r","--setup_recon",dest="setup_recon",help="Setup cortical reconstruction",
		action="store_true",default=False)
	parser.add_option("-s","--setup_stats",dest="setup_stats",
		help="Make spm (stats) batch files using rp_*.txt as mult regressors",action="store_true",
		default=False)
	parser.add_option("-o","--setup_outliers",dest="setup_outliers",
		help="Make spm (stats) batch files using outliers as mult regressors",action="store_true",
		default=False)
	parser.add_option("--study",dest="single_study",help="Setup single study",action="store",
		type="string",default=None)
	parser.add_option("--run_outliers",dest="run_outliers",help="Run outlier script",
		action="store_true",default=False)
	parser.add_option("--run_stats",dest="run_stats",help="Run stats script",action="store_true",
		default=False)
	parser.add_option("--run_preproc",dest="run_preproc",help="Run preproc script",
		action="store_true",default=False)
	parser.add_option("--run_recon",dest="run_recon",help="Run reconstruction script",
		action="store_true",default=False)
	parser.add_option("--setup_bem",dest="setup_bem",help="Setup BEM folders/dicoms",
		action="store_true",default=False)
	parser.add_option("--scan_only",dest="scan_only",help="Run only the scan_only unpack",
		action="store_true",default=False)
	parser.add_option("--scan2cfg",dest="scan2cfg",help="Convert scan.log into cfg file",
		action="store_true",default=False)
	parser.add_option("--unpack",dest="unpack",help="Run the full unpacking step",
		action="store_true",default=False)
	parser.add_option("--parallel",dest="parallel",help="Make the script so jobs run in parallel",
		action="store_true",default=False)
	parser.add_option("--preProc",dest="preProc",help="Run the preProc script for meg",
		action="store_true",default=False)
	parser.add_option("--preAnat",dest="preAnat",help="Run the preAnat script for meg",
		action="store_true",default=False)
	parser.add_option("--launchpad",dest="launchpad",action="store_true",default=False,
		help="Use with --run_*, pipeline only exits when all jobs finish")
	parser.add_option("--setup_second",dest="setup_second",action="store_true",default=False,
		help="Setup second level stats")
	parser.add_option("--surf_second",dest="surf_second",action="store_true",default=False,
		help="Analyze second level stats (that have been run) with Freesurfer")
	parser.add_option("--package_second",dest="package_second",action="store_true",default=False,
		help="Make analysis package (run after --surf_second)")
	parser.add_option("--pvalue",dest="pvalue",type="float",default=0.001,
		help="Use with --surf_second")
	parser.add_option("--date",dest="date",type="string",default=None,
		help="Specify a previously run second level date in the form of YYYYMMDD")
	parser.add_option("--aparc",dest="aparc",action="store_true", default=False,
		help="If passed with --surf_second, tksurfer will load aparc annotations")
	parser.add_option("--dry",dest="dry",action="store_true",default=False,
		help="If passed with --surf_second, all scripts are copied but tksurfer isn't run")
	parser.add_option("--makeInv",dest="makeInv",action="store_true",default=False,
		help="Run the MEG makeInv script")
	parser.add_option("--exc",dest="bad_sub",action="append",default=[],
		help="Exclude subjects, for use with --setup_second")
	parser.add_option("--mask",dest="mask",action="append",default=[],
		help="At the second level, mask certain contrasts. Pass in <study>,<contrast>,<pathtomaskimage>")
	parser.add_option("--inc",dest="good_sub",action="append",default=[],
		help="At the second level, include subjects, use with --setup_second")
	parser.add_option("--run_ica",dest="run_ica",action="store_true",default=False,
		help="Run ICA processing (takes 90-120 minutes).")
	parser.add_option("--cfg2info",dest="cfg2info",action="store_true",default=False,
		help="Convert cfg to info dictionary")
	parser.add_option("--run_second",dest="run_second",action="store_true",default=False,
		help="Run the script made my --setup_second")
	parser.add_option("--override_p",dest="override_p",action="store_true",default=False,
		help="Overwrite p-value with new --pvalue")
	parser.add_option("--all_second",dest="all_second",action="store_true",default=False,
		help="Run --setup_second,--run_second,--surf_second, and --package_second all in one call")
		
	(options,args) = parser.parse_args()
	data = dict({})
	#add options to data
	for opt,value in options.__dict__.items():
		data[opt] = value
	
	#transform parallel
	if data["parallel"]:
		data["parallel"] = "&"
	else:
		data["parallel"] = ""
	#transform aparc
	if data["aparc"]:
		data["aparc"] = "-aparc"
	else:
		data["aparc"] = ""
	#transform mask(s)
	if data["mask"]:
		masks = []
		for mask_str in data["mask"]:
			parts = mask_str.split(",")
			masks.append(parts)
		data["mask"] = masks
	data['stype'] = subject_type(args)
	for subject in args:
		try:
			data["subject"] = subject
			set_paths(data)
			#subject level options
			if data["copy_dicom"]:		
				archive_to_cluster(data)
			if data["scan_only"]:
				scan_only(data)
			if data["scan2cfg"]:
				scan2cfg(data)
			if data["cfg2info"]:
				cfg2info(data)
			if data["unpack"]:
				unpack(data)
			if data["unpack_all"]:
				unpack_all(data)
			if data["setup_preproc"]:
				setup_spm(data,"preproc")
			if data["setup_stats"]:
				setup_spm(data,"stats")
			if data["setup_outliers"]:
				setup_spm(data,"stats_outliers")
			if data["setup_recon"]:
				setup_recon(data)
			if data["run_preproc"]:
				run_mri_script(data,"preproc")
			if data["run_stats"]:
				run_mri_script(data,"stats")
			if data["run_outliers"]:
				run_mri_script(data,"stats_outliers")
			if data["run_recon"]:
				run_mri_script(data,"recon")
			if data["setup_bem"]:
				setup_bem(data)
			if data["preProc"]:
				meg_script(data,"preProc")
			if data["preAnat"]:
				meg_script(data,"preAnat")
			if data["makeInv"]:
				meg_script(data,"makeInv")
			if data["run_ica"]:
				run_ica(data)
		except Exception as strerr:
			print(strerr)
			print("Something happened for {0}".format(data["subject"]))
	#group level options
	if data["all_second"]:
		second_level(data,"setup")
		second_level(data,"run")	
		second_level(data,"surf")
		second_level(data,"package")
	if data["setup_second"]:
		second_level(data,"setup")
	if data["run_second"]:
		second_level(data,"run")
	if data["surf_second"]:
		second_level(data,"surf")
	if data["package_second"]:
		second_level(data,"package")
	#miscellaneous
	if data["launchpad"]:
		pipeline.wait_to_finish(running_jobs)


if __name__ == "__main__":
	main()
