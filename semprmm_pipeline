#!/usr/bin/env python

"""
Semantic Priming (MultiModal) Pipeline
Author: Scott Burns <sburns AT nmr DOT mgh DOT harvard DOT edu>
Copyright 2011 Kuperberg Lab
License: BSD,3 clause
"""

from optparse import OptionParser,OptionGroup
import os
from pprint import pprint
from glob import glob
import getpass
import stat
import sys
import time
try:
	from joblib import Parallel, delayed
	use_joblib = True
except ImportError:
	use_joblib = False
import shutil
from readInput import readTable
import pipeline as pipeline

VER = "1.0"
possible_studies = dict({"ATLLoc": 1, "MaskedMM": 2, "BaleenMM": 8, "AXCPT": 2})
batch_dir = "/cluster/kuperberg/SemPrMM/MRI/scripts/spm_batches/"
func_dir = "/cluster/kuperberg/SemPrMM/MRI/functionals/"
dicom_dir = "/cluster/kuperberg/SemPrMM/MRI/dicoms/"
meg_dir = "/cluster/kuperberg/SemPrMM/MEG/data/"
meg_scripts = "/cluster/kuperberg/SemPrMM/MEG/scripts/"
mri_scripts = "/cluster/kuperberg/SemPrMM/MRI/scripts/"
mri_vtsd = "/cluster/kuperberg/SemPrMM/MRI/vtsd_logs/"
running_jobs = []


class ProgrammerError(Exception):
	pass


class UserError(Exception):
	pass


class SystemError(Exception):
	pass


def archive_to_cluster(data):
	"""
	data: dict
		req. keys:"subject","subject_dicom"
	Copies dicom images from archive dir to dicom dir.
	parallel subjects : YES
	"""
	if data["verbose"]:
		print("archive_to_cluster:")
	data["archive_dir"] = pipeline.find_session([data["subject"], "-x", "Kuperberg"])
	if data["archive_dir"] is None:
		print("ALERT:findsession returned nothing for {0}".format(data["subject"]))
		return
	if not os.path.exists(data["dicom_dir"]):
		os.mkdir(data["dicom_dir"])
	pipeline.mirror(data["archive_dir"], data["dicom_dir"], True)


def scan_only(data):
	"""
	data: dict with keys:"subject_dir","subject_dicom"
	Runs unpacksdcmdir in -scanonly mode, which creates data["mri_dir"]/scan.log
	parallel subjects: NO
	"""
	if not os.path.exists(data["mri_dir"]):
		os.mkdir(data["mri_dir"])
	data["scan_path"] = scan_path(data)
	try:
		pipeline.scan_only(data["dicom_dir"], data["mri_dir"], data["scan_path"])
	except OSError:
		raise UserError("scan_only: start nmrenv and try again.")


def unpack(data):
	"""
	data: dict with keys:"subject_dir", "subject_dicom", "subject"
	Runs unpacksdcmdir in the full mode.
	Can take a long time,  run with care.
	parallel subjects: NO
	"""
	data["cfg_path"] = cfg_path(data)
	if os.access(data["cfg_path"], os.R_OK):
		pipeline.unpack(data["dicom_dir"], data["mri_dir"], data["cfg_path"])
	else:
		print("ALERT: cannot find cfg file for {0}, re-run --scan2cfg".format(data["subject"]))


def cfg_path(data):
	"""
	data: dict with keys:"subject_dir"
	Returns path to cfg file.
	"""
	return os.path.join(data["mri_dir"], "cfg.txt")


def scan_path(data):
	"""
	data: dict with "subject_dir"
	returns path to scan.log file
	"""
	return os.path.join(data["mri_dir"], "scan.log")


def unpack_all(data):
	"""
	data: dict with keys:"subject_dir", "subject_dicom", "subject"
	Wrapper for all three pieces of unpacking a subject"s dicom dir.
	Runs:
		1) scan_only
		2) scan2cfg
		3) cfg2info
		4) unpack
	parallel subjects: NO
	"""
	scan_only(data)
	scan2cfg(data)
	cfg2info(data)
	unpack(data)


def scan2cfg(data):
	"""
	data: dict with keys:"subject_dir"
	Converts scan.log to appropriate cfg file and writes to disk.
	"""
	data["scan_path"] = scan_path(data)
	data["cfg_path"] = cfg_path(data)
	dir_map = dict({"MEMPRAGE_4e_p2_1mm_iso": "MPRAGE", "ge_functionals_atlloc": "ATLLoc",
			"field_mapping": "FieldMap", "ge_functionals_maskmm": "MaskedMM", 
			"ge_functionals_baleen": "BaleenMM", "MEFLASH_8e_1mm_iso_5deg": "MEFLASH",
			"ge_functionals_axcpt": "AXCPT", "ge_functionals_axcpt_sc": "AXCPT"})
	scan_list = [["MEMPRAGE_4e_p2_1mm_iso","ok","256","256","1"], 
				["ge_functionals_atlloc","ok","160"],["ge_functionals_maskmm","ok","148"],
				["ge_functionals_baleen","ok","130"],["ge_functionals_axcpt","ok","240"],
				["MEFLASH_8e_1mm_iso_5deg","256","256","ok","1"],["field_mapping","ok","1"],
				["ge_functionals_axcpt_sc","ok","140"]]
	good_lines = pipeline.scan_to_cfg(dir_map,dir_map,scan_list,data["scan_path"])
	good_lines.sort(key=lambda x:int(x.split()[0]))
	fieldmaps = [v for v in good_lines if v.split()[1] == "FieldMap"]
	if len(fieldmaps) % 2 == 1:
		print("Odd amount of fieldmaps. --scan2cfg cannot continue. Copy/paste/edit as needed.")
		for line in good_lines:
			print(line)
		return
	good_fieldmaps = []
	func_names = ["MaskedMM","BaleenMM","ATLLoc","AXCPT"]
	for fm in fieldmaps:
		ind = good_lines.index(fm)
		all_prev_func = [r for r in good_lines[0:ind] if r.split()[1] in func_names]
		if len(all_prev_func) < 1:
			print("ALERT: odd field maps. Copy/paste/edit the dump below.")
			for line in good_lines:
				print(line)
			return
		prev_func = all_prev_func[len(all_prev_func) - 1]

		func = prev_func.split()[1]
		parts = fm.split()
		(before,dot,filetype) = parts[3].partition(".")
		(nothing,maptext,num) = before.partition("FieldMap")
		if int(num)%2 == 1:
			postfix = "Mag"
		else:
			postfix = "Phase"
		good_fieldmaps.append("{0} {1} {2} FieldMap_{3}_{4}.{5}".format(parts[0],parts[1],parts[2],
			func,postfix,parts[2]))
	not_fieldmaps = [v for v in good_lines if v.split()[1] != "FieldMap"]
	for fm in good_fieldmaps:
		not_fieldmaps.append(fm)
	good_lines = not_fieldmaps
	good_lines.sort(key=lambda x:int(x.split()[0]))
	try:
		write_file_with_list(data["cfg_path"],"\n".join(good_lines),data["verbose"])
	except:
		print("Failure in scan2cfg")
		raise


def makeMC(data):
	codes = {"ATLLoc":{
				"1":("Sentences",False),
				"2":("Wordlist",False),
				"3":("Nonwords",False),
				"duration": "4",
				"on_sub":0.4},
			"BaleenMM":{
				"1":("LP_RelatedTarget",False),
				"2":("LP_UnrelatedTarget",False),
				"4":("LP_UnrelatedFiller",False),
				"5":("LP_AnimalTarget",True),
				"6":("HP_RelatedTarget",False),
				"7":("HP_UnrelatedTarget",False),
				"8":("HP_RelatedFiller",False),
				"9":("HP_UnrelatedFiller",False),
				"10":("HP_AnimalTarget",True),
				"11":("LP_AnimalPrime",True),
				"12":("HP_AnimalPrime",True),
				"duration": "2",
				"on_sub":1},
			"MaskedMM":{
				"1":("DirectlyRelated",False),
				"2":("IndirectlyRelated",False),
				"3":("Unrelated",False),
				"4":("InsectPrime",True),
				"5":("InsectTarget",True),
				"duration": "2",
				"on_sub":1},
			"AXCPT":{
				"1":("AY",False),
				"2":("BX",False),
				"3":("BY",False),
				"4":("AX",True),
				"duration": "2",
				"on_sub": 0}
			}
	#first, just glob all the vtsd_logs from mri_vtsd/data["subject"]
	vtsd_logs = glob(os.path.join(mri_vtsd,data["subject"],"*.vtsd_log"))
	info = pipeline.load_data(info_path(data),data["verbose"])
	for vtsd_log in vtsd_logs:
		if data["verbose"]:
			print("Parsing {0}".format(vtsd_log))
		vtsd_data = readTable(vtsd_log)
		pass
		#study scanner subject list run onset # stims..... code item iti response_time
		code_ind = len(vtsd_data[0]) - 1 - 3
		response_ind = len(vtsd_data[0]) - 1
		iti_ind = len(vtsd_data[0]) - 1 - 1
		onset_ind = 5
		run = vtsd_data[0][4]
		study = vtsd_data[0][0]
		sub = codes[study]["on_sub"]
		uncodes = sorted(set([x[code_ind] for x in vtsd_data]),cmp=lambda x,y: cmp(int(x),int(y)))
		misses = []
		for code in uncodes:
			#get raw data
			good_onsets = []
			for trial in [x for x in vtsd_data if x[code_ind] == code]:
				task = codes[study][code][1]
				response =  trial[response_ind] != "0.000"
				if (task and response) or (not task and not response):
					good_onsets.append(trial[onset_ind])
				if (not task and response) or (task and not response):
					misses.append(trial[onset_ind])
			#xfm to floats, subtract, round,int,back to string
			xfm_onsets = map(str,[int(round(float(x) - sub)) for x in good_onsets])
			if len(xfm_onsets) == 0:
				print("WARNING: empty onsets for {0}.Please fix manually".format(codes[study][code][0]))
			new_dict = {"Run"+run+codes[study][code][0]+"Onsets": " ".join(xfm_onsets),
				"Run"+run+codes[study][code][0]+"Durations": " ".join(codes[study]["duration"] * len(xfm_onsets))}
			if study in info:
				info[study].update(new_dict)
		#do misses
		if len(misses) > 0:
			xfm_misses = [str(int(round(float(x) - sub))) for x in misses]
			miss_dur = [codes[study]["duration"]] * len(xfm_misses)
			print("{0}:{1}:Run{2}:{3} miss(es)".format(data["subject"],study,run,len(xfm_misses)))
		else:
			trials_with_iti = [x for x in vtsd_data if x[iti_ind] == "2.000"]

			xfm_misses = [str(int(round(float(trials_with_iti[0][onset_ind]))+float(codes[study]["duration"])))]
			miss_dur = ["2"]
		info[study].update({"Run"+run+"MissesOnsets": " ".join(xfm_misses),
					"Run"+run+"MissesDurations": " ".join(miss_dur)})
	pipeline.save_data(info,info_path(data),data["verbose"])


def info_path(data):
	"""
	just a little method to encapsulate this path
	"""
	return os.path.join(data["mri_dir"],"info.txt")


def cfg2info(data):
	"""
	converts the cfg.txt to a pickled data file. This data file is used in the rest of the stream 
	and as such, is very important.
	The pickled data is a dictionary with keys of the studies that were run and values that are 
	dictionaries themselves.  These dictionaries hold keys like "MPRAGEXXX","FieldMapPhaseXXX",etc. 
	and are used as the keyword mapping when templating SPM batches.
	"""
	cfg_fname = cfg_path(data)
	if not os.path.exists(cfg_fname):
		print("ALERT: cannot find cfg file for {0}".format(data["subject"]))
		raise UserError
	cfg = readTable(cfg_fname)
	info = dict({})
	prepend_zero = lambda x: "00"+x if int(x)<10 else "0"+x
	for study in possible_studies:
		study_dict = dict({})
		runs = [x for x in cfg if x[1] == study]
		if len(runs) > 0:
			study_dict["was_run"] = True
			for run in runs:
				run_num = run[3].split(".")[0].split(study)[1]
				study_dict["Run"+run_num+"XXX"] = prepend_zero(run[0])
			if not (study == "AXCPT" and data["stype"] == "ac"):
				if len(runs) == possible_studies[study]:
					study_dict["complete"] = True
				else:
					study_dict["complete"] = False
			else:
				if len(runs) == 3:
					study_dict["complete"] = True
				else:
					study_dict["complete"] = False
			#find the MPRAGE
			mprage_runs = [x for x in cfg if x[1] == "MPRAGE"]
			study_dict["MPRAGE_runs"] = [prepend_zero(x[0]) for x in mprage_runs]
			#find the Phase and Mag XXX
			for type in ["Phase","Mag"]:
				type_run = [x for x in cfg if study in x[3] and type in x[3]]
				if not len(type_run) == 1:
					print("ALERT: Couldn't find the FieldMap_{0}_{1}.nii in cfg for {2}".format(study,
						type,data["subject"]))
				else:	
					study_dict["FieldMap"+type+"XXX"] = prepend_zero(type_run[0][0])
			info[study] = study_dict
		else:
			study_dict["was_run"] = False
	#write out info.txt as pickled dict
	pipeline.save_data(info,info_path(data),data["verbose"])
	if data["verbose"]:
		pprint("Info:")
		pprint(info)
		
	
def studies_to_setup(data,type):
	"""
	returns what studies should be set up. This list depends on the type of processing to be setup.
	"""
	if not os.path.exists(info_path(data)):
		print("No info.txt for {0}, rerun --cfg2info".format(data["subject"]))
		raise UserError("Broke in studies_to_setup")
	info = pipeline.load_data(info_path(data),data["verbose"])
	if data["single_study"]:
		studies = [data["single_study"]]
	else:		
		if "stats" in type or "art" in type:
			studies = [k for k,v in info.iteritems() if os.path.exists(
				touch_file_path(data,k,"preproc","","run"))]
		elif "preproc" in type:
			studies = [k for k,v in info.iteritems() if v["was_run"]]
	return studies


def setup_spm(data,type):
	"""
	Any 1st-level mri processing goes through this function.
	"""
	#does this subject exist?
	if not os.path.exists(data["mri_dir"]):
		print("Hey dummy, no MRI dir for {0}".format(data["subject"]))
		raise UserError("Broke in setup_spm")
	data["studies_to_setup"] = studies_to_setup(data,type)
	for study in data["studies_to_setup"]:
		job_dir = os.path.join(data["mri_dir"],study,"jobs")
		if not os.path.exists(job_dir):
			os.mkdir(job_dir)
		if "stats" in type:
			stat_dirs = []
			if study != "BaleenMM":
				stat_dirs.append(os.path.join(data["mri_dir"],study,type))
			else:
				for block in ["1","2"]:
					stat_dirs.append(os.path.join(data["mri_dir"],study,type+block))
			for smooth in ["6mm","8mm"]:
				for stat_dir in stat_dirs:
					if not os.path.exists(stat_dir):
						os.mkdir(stat_dir)
					smooth_dir = os.path.join(stat_dir,smooth)
					if not os.path.exists(smooth_dir):
						os.mkdir(smooth_dir)
		if "stats" in type and study == "BaleenMM":
			write_mlab_script(data,study,type,"1")
			write_mlab_script(data,study,type,"2")
		else:
			write_mlab_script(data,study,type,"")
	write_shell_script(data,type)


def shell_script_path(data,type):
	"""
	This function just encapsulates the little bit of logic needed to name a shell script.
	"""
	script_dir = os.path.join(data["mri_dir"],"scripts")
	if not os.path.exists(script_dir):
		os.mkdir(script_dir)
	if "recon" in type:
		return os.path.join(script_dir,"recon"+"_"+data["subject"]+".sh")
	elif data["single_study"]:
		return os.path.join(script_dir,data["single_study"].lower()+"_"+type+"_"+data["subject"]+".sh")
	else:
		return os.path.join(script_dir,"all_"+type+"_"+data["subject"]+".sh")


def write_shell_script(data,type):
	"""
	Writes out the shell script with matlab calls if we're preprocessing or stats processing or with
	recon-all functions if we're reconstructing.
	If --parallel was used, then & will be appended to the matlab lines. With the new parallelization
	scheme I've got with joblib, this could be dangerous, use at your own risk.
	"""
	shell_script = shell_script_path(data,type)
	commands = []
	commands.append("#!/bin/sh")
	if "recon" in type:
		setup_recon = "recon-all -s " + data["subject"]
		#find all mprages 
		mprages = glob(os.path.join(data["mri_dir"],"MPRAGE","*","MPRAGE*.nii"))
		for mprage in mprages:
			setup_recon = setup_recon + " -i " + mprage
		commands.append(setup_recon + " > {0}".format(os.path.join(data["mri_dir"],"scripts","setup_recon.log")))
		commands.append("nohup recon-all -all -s {0} -mail {1} >& /dev/null &".format(data["subject"],getpass.getuser()))
	else:
		if "stats" in type:
			commands.append("unset DISPLAY")
		mlab_cmd = "nohup matlab7.11 -nosplash -nodesktop"
		if "stats" in type:
			mlab_cmd += " -nodisplay "
		if not "studies_to_setup" in data:
			data["studies_to_setup"] = studies_to_setup(data,type)
		for study in data["studies_to_setup"]:
			if study == "BaleenMM" and "stats" in type:
				job_file = os.path.join(data["mri_dir"],study,"jobs",study.lower()+"_"+
					type+"_block1.m")
				log_file = os.path.join(data["mri_dir"],study,"jobs",study.lower()+"_"+
					type+"_block1.log")
				commands.append("{2} < {0} > {1} {3}".format(job_file,log_file,
					mlab_cmd,data["parallel"]))
				job_file = os.path.join(data["mri_dir"],study,"jobs",study.lower()+"_"+
					type+"_block2.m")
				log_file = os.path.join(data["mri_dir"],study,"jobs",study.lower()+"_"+
					type+"_block2.log")
				commands.append("{2} < {0} > {1} {3}".format(job_file,log_file,
					mlab_cmd,data["parallel"]))
			else:
				job_file = os.path.join(data["mri_dir"],study,"jobs",study.lower()+"_"+type+".m")
				log_file = os.path.join(data["mri_dir"],study,"jobs",study.lower()+"_"+type+".log")
				commands.append("{2} < {0}  > {1} {3}".format(job_file,log_file,
					mlab_cmd,data["parallel"]))
	try:
		write_file_with_list(shell_script,"\n".join(commands),data["verbose"])
	except IOError:
		print(commands)
		raise SystemError("Cannot write shell script. See text above traceback for shell commands.")
	make_file_exec(shell_script)		

def run_art(data):
	"""
	This function writes out the ART-specific session file, then a short matlab script, then
	runs matlab and passes the script into the matlab process to run. We use ART defaults.
	"""
	studies = studies_to_setup(data,"art")
	sessions = []
	for study in studies:
		#glob the images
		images = glob(os.path.join(data["mri_dir"],study,"*",study+"*.nii"))
		motions = glob(os.path.join(data["mri_dir"],study,"*","rp_"+study+"*.txt"))
		if len(images) != len(motions):
			print("Not the same amount of images and motion files")
			raise UserError("run_art")
		sess_contents = []
		sess_contents.append("sessions: {0}".format(len(images)))
		sess_contents.append("global_mean: 1")
		sess_contents.append("drop_flag: 0")
		sess_contents.append("motion_file_type: 0")
		sess_contents.append("end")
		for i,(im,mo) in enumerate(zip(images,motions)):
			sess_contents.append("session {0} image {1}".format(str(i+1),im))
			sess_contents.append("session {0} motion {1}".format(str(i+1),mo))
		sess_contents.append("end")
		sess_path = os.path.join(data["mri_dir"],study,"art_sess.txt")
		sessions.append(sess_path)
		write_file_with_list(sess_path,"\n".join(sess_contents),data["verbose"])
	art_m = []
	for session in sessions:
		art_m.append("art('sess_file','{0}');".format(session))
	art_m.append("exit;")
	art_path = os.path.join(data["mri_dir"],"run_art.m")

	write_file_with_list(art_path,"\n".join(art_m),data["verbose"])
	my_args = ["matlab7.11", "-nodisplay","-nosplash","-nodesktop", "<", art_path]
	proc = pipeline.run_process(my_args,output=sys.stdout,error=sys.stderr)
	proc.communicate()
	print("Finished ART for {0}".format(data["subject"]))


def make_file_exec(path):
	"""
	just makes the file given by path executable by the system
	"""
	os.chmod(path,stat.S_IRUSR|stat.S_IWUSR|stat.S_IXUSR|stat.S_IRGRP|stat.S_IWGRP|stat.S_IXGRP|stat.S_IROTH )
	
	
def specific_batch_path(data,study,type,block):
	"""
	another little encapsulating method
	"""
	if study == "BaleenMM" and "stats" in type:
		return os.path.join(data["mri_dir"],study,"jobs",study.lower()+"_"+
			type+"_block"+block+"_job.m")
	else:
		return os.path.join(data["mri_dir"],study,"jobs",study.lower()+"_"+type+"_job.m")
		

def write_mlab_script(data,study,type,block):
	"""
	This basically just wraps f2f_replace and uses the dictionary from matlab_batch_dict.
	"""
	if "stats_outliers" in type:
		good_type = "stats"
	else:
		good_type = type
	batch_dict = matlab_batch_dict(data,study,type,block)
	if study == "BaleenMM" and "stats" in type:
		if block is None:
			raise ProgrammerError("write_batch_files: no block specified for baleenmm stats")
		batch_o = os.path.join(data["mri_dir"],study,"jobs",study.lower()+"_"+
			type+"_block"+block+".m")
		batch_i = os.path.join(batch_dir,data["stype"],study.lower()+"_"+good_type+"_block"+block+".m")
	else:
		batch_o = os.path.join(data["mri_dir"],study,"jobs",study.lower()+"_"+type+".m")
		batch_i = os.path.join(batch_dir,data["stype"],study.lower()+"_"+good_type+".m")
	pipeline.f2f_replace(batch_i,batch_o,batch_dict,data["verbose"])


def matlab_batch_dict(data,study,type,block):
	"""
	Adds to the dictionary returned by build_funcdir_info.  Any keywords you insert to the SPM batches
	should be added here.
	"""
	replace_dict = build_funcdir_info(data,study,type,block)
	replace_dict["study"] = study.lower()
	replace_dict["type"] = type
	if block:
		replace_dict["block"] = "_block"+block
	else:
		replace_dict["block"] = "<none>"
	if "stats" in type:
		replace_dict["SixSPM"] = os.path.join(data["mri_dir"],study,type,"6mm","SPM.mat")
		replace_dict["EightSPM"] = os.path.join(data["mri_dir"],study,type,"8mm","SPM.mat")
	if block:
		replace_dict["SixSPM"] = os.path.join(data["mri_dir"],study,type+block,"6mm","SPM.mat")
		replace_dict["EightSPM"] = os.path.join(data["mri_dir"],study,type+block,"8mm","SPM.mat")
	replace_dict["run_file"] = touch_file_path(data,study,type,block,"run")
	replace_dict["start_file"] = touch_file_path(data,study,type,block,"start")
	replace_dict["email_success"] = "{0} {1} {2}{3} succeeded".format(data["subject"],study,
		type,block)
	replace_dict["email_fail"] =  "{0} {1} {2}{3} failed".format(data["subject"],study,
		type,block)
	return replace_dict

def print_info(data):
	info = pipeline.load_data(info_path(data))
	for k,v in sorted(info.items()):
		print("{0}...".format(k))
		for key,value in sorted(v.items()):
			print("{0}:{1}".format(key,value))
		print('\n')

def build_funcdir_info(data,study,type,block):
	"""build_funcdir_info:
	looks for info.txt and uses the dictionary stored that for everything.
	The MPRAGEXXX is filled out here"""
	info = pipeline.load_data(info_path(data),data["verbose"])
	study_dict = info[study];
	study_dict["subject"] = data["subject"]
	study_dict["stat_folder"] = type+block
	if "stats" in type:
		# we need to insert MRs into study_dict
		run_keys = [k for k,v in study_dict.iteritems() if "Run" in k]
		for run_key in run_keys:
			mr_key = run_key.split("XXX")[0]
			run_num = mr_key.split("Run")[1]
			mr_key += "MR"
			if "outliers" in type:
				study_dict[mr_key] = "art_regression_outliers_and_movement_{0}{1}.mat".format(
					study,run_num)
			else:
				study_dict[mr_key] = "rp_{0}{1}.txt".format(study,run_num)
	elif type == "preproc":
		#find mprage xxx (hardcoded to use the first)
		study_dict["MPRAGEXXX"] = study_dict["MPRAGE_runs"][0]
	return study_dict	


def touch_file_path(data,study,type,block,when):
	"""
	yet another little method to capture this logic in one place
	"""
	if block:
		s = os.path.join(data["mri_dir"],study,"jobs",study.lower()+"_"+type+"_block"+block+"_"+when)
	else:
		s = os.path.join(data["mri_dir"],study,"jobs",study.lower()+"_"+type+"_"+when)
	return s


def run_mri_script(data,type):
	"""
	Looks for the write script to run based on type and executes it. This method blocks which is ok
	because this method is starting in parallel automatically for many subjects. This will block 
	even if you used --parallel with --setup_*.
	"""
	#find all the scripts of this type
	if type == "recon":
		scripts = [shell_script_path(data,type)]
	else:
		if data["single_study"]:
			prefix = data["single_study"].lower()+"_"
		else :
			prefix = "all_"
		scripts = glob(os.path.join(data["mri_dir"],"scripts",prefix+type+"_*"))
	for script in scripts:
		args = ["nohup", script]
		print("\nStarting {0}\n".format(script))
		pipeline.run_process(args).wait()


def new_preProc(data):
	"""
	New pure-python pre-processing for MEG.
	"""
	import meg_py
	print("Starting meg_py:preProc")
	meg_py.preProc(data["subject"],data["meg_dir"])


def meg_script(data,type,extra=None):
	"""
	Run a particular meg script as given in type.
	data: the regular dict
	type: name of meg script (e.g. "preProc1","preAnat1",etc)
	extra: list of extra arguments (e.g. for "preProc2", extra would be [{preBlinkTime},
	{postBlinkTime}])
	"""
	script_path = os.path.join(meg_scripts,type+".sh")
	if not os.path.exists(script_path):
		raise UserError("meg_script: can not locate {0}".format(type))
	my_args = [script_path,data["subject"]]
	if type == "preProc":
		#look for fif
		fif = glob(os.path.join(data["meg_dir"],"*_raw.fif"))
		#filter our blink and emptyroom
		fif = [x for x in fif if x.find("Blink") == -1 and x.find("emptyroom") == -1]
		N_fif = len(fif)
		if data["verbose"]:
			print("Fifs:\n{0}".format("\n".join(fif)))
		icamat = glob(os.path.join(data["meg_dir"],"*_raw_ica.mat"))
		N_icamat = len(icamat)
		if data["verbose"]:
			print("ICA mat:\n{0}".format("\n".join(icamat)))
		blinks = glob(os.path.join(data["meg_dir"],"*.blinks"))
		N_blinks = len(blinks)
		if data["verbose"]:
			print("Blinks:\n{0}".format("\n".join(blinks)))
		if (N_fif == 0):
			raise UserError("No fif files found for this subject")
		if not (N_fif == N_icamat == N_blinks):
			raise UserError("Unequal amount of fif,ica.mat,and blink files.\nTry Again")
		# DEFAULT EYE BLINK ARGUMENTS FOR PREPROC1
		extra = ["-0.1","0.3"]
	if type == "preAnat":
		setup_bem(data)
		#check that flash was unpacked
		if os.path.exists(os.path.join(data["mri_dir"],"MEFLASH")):
			extra = ["FLASH"]
		else:
			extra = ["WATER"]
	if extra:
		my_args.extend(extra)
	#start the process 
	log_file = "{0}{1}_{2}.log".format(data["meg_dir"],data["subject"],type)
	print("Logging to {0}".format(log_file))
	f = open(log_file,"w")
	process = pipeline.run_process(my_args,output=f,error=f)
	if data["parallel"]:
		running_jobs.append(process)	
	else:
		process.communicate()[0]


def run_ica(data):
	"""
	just writes out the run_ica script for a subject and runs it.
	"""
	iscript = os.path.join(meg_scripts,"generic_runica.m")
	oscript = os.path.join(data["meg_dir"],data["subject"]+"_ica.m")
	pipeline.f2f_replace(iscript,oscript,dict({"subject": data["subject"]}),data["verbose"])
	log = os.path.join(data["meg_dir"],data["subject"]+"_ica.log")
	my_args = ["matlab7.11", "-nodisplay","-nosplash","-nodesktop", "<", oscript,
		">", log]
	try:
		process = pipeline.run_process(my_args,output=None)
		process.communicate()
	except:
		print("Unexpected error when running ICA")
		raise


def setup_bem(data):
	"""
	This makes symbolic links to the MEFLASH_8e_1mm_iso_5deg dicom images. This is called for --preProc.
	"""
	print("Setup_BEM:{0}".format(data["subject"]))
	#must have env var set
	if os.getenv("SUBJECTS_DIR") is None:
		raise UserError("$SUBJECTS_DIR is not set")
	recon_dir = os.path.join(os.getenv("SUBJECTS_DIR"),data["subject"])
	#if the recon hasn"t run yet quit
	if not os.path.exists(recon_dir):
		raise UserError("Reconstruction folders not setup. Start recon and try again later.")
	bem_dir = os.path.join(recon_dir,"bem")
	if not os.path.exists(bem_dir):
		os.mkdir(os.path.join(recon_dir,"bem"))
	flash_dcm_dir = os.path.join(bem_dir,"flash_dcm")
	if not os.path.exists(flash_dcm_dir):	
		os.mkdir(flash_dcm_dir)
	flash_org_dir = os.path.join(bem_dir,"flash_org")	
	if not os.path.exists(flash_org_dir):	
		os.mkdir(flash_org_dir)
	try:
		scanlog_path = scan_path(data)
		all_lines = list_from_file(scanlog_path,data["verbose"])
		dcm = [line.split()[7] for line in all_lines if "MEFLASH_8e_1mm_iso_5deg" in line and " 8 " in line and " err " not in line]
		if len(dcm) > 1:
			ProgrammerError("More than one MEFLASH5 with 8 frames...check scan.log")
		elif len(dcm) == 0:
			dcm = None;		
		else:
			dcm = dcm[0]
	except IOError:
		raise ProgrammerError("Could not open scan.log file")
	if dcm:
		(sub_dcm,dash,after) = dcm.rpartition("-")
		search_dir = os.path.join(data["dicom_dir"],sub_dcm+"*.dcm")
		flash_dcms = glob(search_dir)
		flash_dcms.sort()
		for flash_dcm in flash_dcms:
			(path,slash,filename) = flash_dcm.rpartition("/")
			try:
				sympath = os.path.join(flash_dcm_dir,filename)
				os.symlink(flash_dcm,sympath)
			except OSError:
				pass
	else:
		print("NO Flash DCM...")


def set_paths(data):
	"""Fairly self-explanatory.  Adds keys to the dictionary for "mri_dir","subject_dir",
	"dicom_dir",and "subject_dicom".  path prefixes are hard coded.
	"""
	data["mri_dir"] = os.path.join(func_dir,data["subject"])+os.sep
	data["dicom_dir"] = os.path.join(dicom_dir,data["subject"])+os.sep
	data["meg_dir"] = os.path.join(meg_dir,data["subject"])+os.sep


def second_level(data,type):
	"""
	All second level functions pass through here.
	"""
	prefix = os.path.join(func_dir,"SecondLevelStats")
	study_contrasts = dict({"ATLLoc": [["Nonwords","0003"],["WordLists","0002"],["Sentences","0001"],
		["SentencesVWordLists","0004"],["SentencesVNonwords","0006"],["WordListsVNonwords","0008"]],
		"MaskedMM": [["Rel","0001"],["UnRel","0003"],["UnRelVRel","0007"]],
		"BaleenLP": [["Rel","0001"],["UnRel","0002"],["UnRelVRel","0007"]],
		"BaleenHP": [["Rel","0001"],["UnRel","0002"],["UnRelVRel","0008"],
		["UnRelFillvRelFill","0009"]],"AXCPT": [["AYvBY","0009"],["BXvBY","0011"]]})
	study_stats = dict({"ATLLoc": "stats_outliers","MaskedMM": "stats_outliers",
		"BaleenLP": "stats_outliers1","BaleenHP": "stats_outliers2","AXCPT": "stats_outliers"})
	if data["date"]:
		date_dir = data["date"]
	else:
		date_dir = time.strftime("%Y%m%d")
	if type == "setup":
		second_setup(data,prefix,date_dir,study_contrasts,study_stats)
	if type == "run":
		second_run(data,prefix,date_dir,study_contrasts,study_stats)
	if type == "surf":
		second_surf(data,prefix,date_dir,study_contrasts,study_stats)
	if type == "package":
		second_package(data,prefix,date_dir,study_contrasts,study_stats)

		
def second_package(data,prefix,date,study_contrasts,study_stats):
	"""
	This copies images and copy/replaces the HTML files to the date directory.
	"""
	print("Packaging second level surf analysis....\n")
	package_dir = os.path.join(prefix,"surf_analysis_"+date)
	surf_dir = os.path.join(mri_scripts,"surf_analysis")
	if not os.path.exists(package_dir):
		os.mkdir(package_dir)
	if data["single_study"]:
		studies = [data["single_study"]]
	else:
		studies = ["ATLLoc","BaleenLP","BaleenHP","MaskedMM","AXCPT"]
	#copy index.html
	shutil.copyfile(os.path.join(surf_dir,"index.html"),
		os.path.join(package_dir,"index.html"))
	for study in studies:
		date_dir = os.path.join(prefix,data["stype"],study,date)
		#copy <study>.html
		shutil.copyfile(os.path.join(surf_dir,study+".html"),
			os.path.join(package_dir,study+".html"))
		#within package_dir, make study/
		study_dir = os.path.join(package_dir,study)
		if not os.path.exists(study_dir):
			os.mkdir(study_dir)
		for [contrast,XXXX] in study_contrasts[study]:
			print("{0}:\t{1}...".format(study,contrast))
			#within study_dir, make con/
			con_dir = os.path.join(date_dir,contrast)
			res_con_dir = os.path.join(study_dir,contrast)
			if not os.path.exists(res_con_dir):
				os.mkdir(res_con_dir)
			#data_con_dir points to data contrast directory
			img_con_dir = os.path.join(con_dir,"img")
			#within every data_con_dir is which holds all jpgs
			#we want to copy these jpgs to res_con_dir
			print("Copying images...")
			jpgs = glob(os.path.join(img_con_dir,"*.jpg"))
			for jpg in jpgs:
				shutil.copy(jpg,res_con_dir)
			info_fname = os.path.join(con_dir,"info.txt")
			info = pipeline.load_data(info_fname,data["verbose"])
			#now, make the <con>.html file
			replace_dict = dict({"N": str(info["N"]),"pvalue": str(info["pvalue"]),
				"contrast": info["contrast"],"study": study,"stype": info["stype"],
				"images": "\n".join(["<li>{0}</li>".format(x) for x in info["images"]]),
				"mask": info["mask"]})
			pipeline.f2f_replace(os.path.join(surf_dir,"contrast.html"),
				os.path.join(study_dir,contrast+".html"),replace_dict,data["verbose"])

			
def second_surf(data,prefix,date,study_contrasts,study_stats):
	"""
	This copy/replaces the make_images.sh script and runs each one for each paradigm/contrast.
	"""
	from scipy import stats
	import shutil
	print("We're going surfing....\n")
	if data["single_study"]:
		studies = [data["single_study"]]
	else:
		studies = ["ATLLoc","BaleenLP","BaleenHP","MaskedMM","AXCPT"]
	for study in studies:
		date_dir = os.path.join(prefix,data["stype"],study,date)
		for [contrast,XXXX] in study_contrasts[study]:
			print("\n{0}:\t{1}...".format(study,contrast))
			con_dir = os.path.join(date_dir,contrast)
			#first, check that --setup_second has been run by looking for con_dir
			if not os.path.exists(con_dir):
				print("ERROR:--setup_second hasn't been run.")
				print("Do this and try again or pass in a specific date by --date=YYYYMMDD")
				break
			#next, check that SPM has been run and that spmT_0001.img exists
			t_img_exists = len(glob(os.path.join(con_dir,"spmT_0001.img"))) == 1
			if not t_img_exists:
				print("ERROR:SPM hasn't been run on {0}:{1}".format(study,contrast))
				break
			#spmT_0001.img exists, let's proceed
			#look for the pickled data
			info_fname = os.path.join(con_dir,"info.txt")
			info = pipeline.load_data(info_fname,data["verbose"])
			script_dict = dict({})
			if not "pvalue" in info:
				info["pvalue"] = data["pvalue"]
			if data["override_p"]:
				info["pvalue"] = data["pvalue"]
			#else, assume pvalue is already in info
			info["tvalue"] = stats.t.isf(info["pvalue"],info["N"])
			print("T-threshold at N={0} and p<{1} is {2}".format(info["N"],info["pvalue"],
				info["tvalue"]))
			script_dict["tvalue"] = str(info["tvalue"])
			script_dict["aparc"] = data["aparc"]
			script_dict["con_dir"] = con_dir
			new_script_path = os.path.join(con_dir,"make_images.sh")
			pipeline.f2f_replace(os.path.join(mri_scripts,"surf_analysis","make_images.sh"),
				new_script_path,script_dict,data["verbose"])
			make_file_exec(new_script_path)
			#copy lh_tiffs and rh_tiffs
			for surf_script in ["lh_tiff","rh_tiff"]:
				tiffs_path = os.path.join(mri_scripts,"surf_analysis",surf_script)
				new_tiffs_path = os.path.join(con_dir,surf_script)
				shutil.copyfile(tiffs_path,new_tiffs_path)
			pipeline.save_data(info,info_fname,data["verbose"])
			if not data["dry"]:
				# all ready to run make_images.sh
				try:
					process = pipeline.run_process([new_script_path],output=sys.stdout,
						error=sys.stderr)
					process.communicate()[0]
				except KeyboardInterrupt:
					sys.exit("Surf canceled by user...Goodbye")

			
def second_setup(data,prefix,date_dir,study_contrasts,study_stats):
	"""
	This copy/replaces the generic_2nd_level.m script for each paradigm/contrast and writes out a 
	script to run each in matlab. This is the 2nd-level cousin to --setup_spm.
	"""
	mlab_cmd = "matlab7.11 -nodisplay -nosplash -nodesktop "
	shell_commands = []
	shell_commands.append("#!/bin/sh")
	shell_commands.append("unset DISPLAY")
	if data["single_study"]:
		studies = [data["single_study"]]
	else:
		studies = ["ATLLoc","BaleenLP","BaleenHP","MaskedMM","AXCPT"]
	for study in studies:
		new_dir = os.path.join(prefix,data['stype'],study,date_dir)
		#make the new dir
		if not os.path.exists(new_dir):
			os.mkdir(new_dir)
		job_dir = os.path.join(new_dir,"jobs")
		if not os.path.exists(job_dir):
			os.mkdir(job_dir)
		print("\n{0}:\t{1} contrasts".format(study,len(study_contrasts[study])))
		for [contrast,XXXX] in study_contrasts[study]:
			replace_dict = dict()
			replace_dict["contrast"] = contrast
			con_dir = os.path.join(new_dir,contrast)
			replace_dict["con_dir"] = con_dir
			if not os.path.exists(con_dir):
				os.mkdir(con_dir)
			if "Baleen" in study:
				real_study = "BaleenMM"
			else:
				real_study = study
			search_term = os.path.join(func_dir,data["stype"]+"*",real_study,study_stats[study],"8mm",
				"con_"+XXXX+".img")
			all_img = glob(search_term)
			#add included subjecdt images
			if data["good_sub"]:
				for sub in data["good_sub"]:
					all_img.extend(glob(os.path.join(func_dir,sub,real_study,study_stats[study],
						"8mm","con_"+XXXX+".img")))
			#remove excluded subject images
			good_img = [x for x in all_img if len([bad for bad in data["bad_sub"] if bad in x]) == 0]
			good_img[:] = ["'"+x+"'" for x in good_img]
			N = len(good_img)
			replace_dict["contrast_images"] = "\n".join(good_img)
			#are we masking?
			match = [x for x in data["mask"] if x[0] == study and x[1] == contrast]
			if match:
				mask = match[0][2]
				print("Using mask at {0}".format(mask))
			else:
				mask = ""
			replace_dict["mask"] = mask
			replace_dict["email_fail"] = "{0} {1} 2nd Level failed".format(study,contrast)
			replace_dict["SPM"] = "{0}/SPM.mat".format(con_dir)
			ibatch = os.path.join(batch_dir,data["stype"],"generic_2nd_batch.m")
			obatch = os.path.join(job_dir,contrast+".m")
			#make batch
			pipeline.f2f_replace(ibatch,obatch,replace_dict,data["verbose"])
			log = os.path.join(job_dir,contrast+".log")
			shell_commands.append("{0} < {1} > {2} {3}".format(mlab_cmd,obatch,log,data["parallel"]))
			shell_commands.append("echo {0}:{1} has finished.".format(study,contrast))
			info_fname = os.path.join(con_dir,"info.txt")
			info = dict({"N": N,"images": good_img,"stype": data["stype"],"contrast": contrast})
			if mask:
				info["mask"] = mask
			else:
				info["mask"] = "No mask used"
			pipeline.save_data(info,info_fname,data["verbose"])
			print("{0} -> {1} subjects".format(contrast,N))	
	shell_path = second_script(data)
	write_file_with_list(shell_path,"\n".join(shell_commands),data["verbose"])
	make_file_exec(shell_path)


def second_run(data,prefix,date_dir,study_contrasts,study_stats):
	"""
	Runs the script made by --setup_second.  That script's output is hooked into sys.stdout (which 
	probably the terminal)
	"""
	script_path = second_script(data)
	print("\nStarting {0}".format(script_path))
	process = pipeline.run_process(script_path,output=sys.stdout)
	process.communicate()


def second_script(data):
	"""
	returns the path to the second-level script.
	"""
	return os.path.join(func_dir,"SecondLevelStats",data["stype"],"all_studies.sh")

		
def subject_type(subjects):
	"""
	This code is shakey but works.
	"""
	# assume first is good
	if len(subjects) == 1:
		stype = subjects[0][:2]
	else:
		stype = subjects[0][:2]
#		 match = len([x for x in subjects[1:] if x[:2] == stype]) == (len(subjects) - 1)
#		 if not match:
#			 print("\nWarning, you seem to be processing subjects of differing types!!!!\n")
#			 response = 'n'
#			 response = input("Do you wish to continue? (y/n) : ")
#			 if response != "y":
#				 raise UserError("Try again with a better subject list")
	return stype


def write_file_with_list(path,lines,verbose=None):
	"""
	Any file writing should go through here.
	"""
	try:
		with open(path,'w') as f:
			f.writelines(lines)
		if verbose:
			print("Wrote {0}".format(path))
	except IOError:
		raise


def list_from_file(path,verbose=None):
	"""
	Any file reading should go through here. Returns a list from the file split at newlines.
	"""
	try: 
		with open(path,'r') as f:
			all_lines = f.read()
			all_lines = all_lines.splitlines()
			if verbose:
				print("Reading from {0}".format(path))
	except IOError:
		print("Cannot open {0}".path)
		raise
	return all_lines


def process_subject(subject,data):
	"""
	This processes options on a single-subject basis.
	"""
	try:
		data["subject"] = subject
		set_paths(data)
		if data["copy_dicom"]:		
			archive_to_cluster(data)
		if data["scan_only"]:
			scan_only(data)
		if data["scan2cfg"]:
			scan2cfg(data)
		if data["cfg2info"]:
			cfg2info(data)
		if data["unpack"]:
			unpack(data)
		if data["unpack_all"]:
			unpack_all(data)
		if data["setup_preproc"]:
			setup_spm(data,"preproc")
		if data["makeMC"]:
			makeMC(data)
		if data["print_info"]:
			print_info(data)
		if data["run_art"]:
			run_art(data)
		if data["setup_stats"]:
			setup_spm(data,"stats")
		if data["setup_outliers"]:
			setup_spm(data,"stats_outliers")
		if data["setup_recon"]:
			write_shell_script(data,"recon")
		if data["run_preproc"]:
			run_mri_script(data,"preproc")
		if data["run_stats"]:
			run_mri_script(data,"stats")
		if data["run_outliers"]:
			run_mri_script(data,"stats_outliers")
		if data["run_recon"]:
			run_mri_script(data,"recon")
		if data["setup_bem"]:
			setup_bem(data)
		if data["preProc"]:
			meg_script(data,"preProc")
		if data["new_preProc"]:
			new_preProc(data)
		if data["preAnat"]:
			meg_script(data,"preAnat")
		if data["makeInv"]:
			meg_script(data,"makeInv")
		if data["run_ica"]:
			run_ica(data)
	except Exception as strerr:
		print(strerr)

def get_subjects(sub_list):
	raw = list_from_file(sub_list)
	pass
	subjects = []
	for s in raw:
		if s and s[0] != "#": #notempty
			subjects.append(s)
	return subjects


if __name__ == "__main__":
	parser = OptionParser(usage="%prog [options] subject (more subjects)", version="%prog 1.0")

	#Copying/Unpacking options
	copy_group = OptionGroup(parser,"Copying/Unpacking")
	copy_group.add_option("--copy_dicom",dest="copy_dicom", 
		help="Copy from archive to $DICOM_DIR/[subject]",action="store_true",default=False)
	copy_group.add_option("--unpack_all",dest="unpack_all",action="store_true",default=False,
		help="If specified, unpacking occurs in $FUNCTIONAL_DIR/[subject]")
	copy_group.add_option("--scan_only",dest="scan_only",help="Run only the scan_only unpack",
		action="store_true",default=False)
	copy_group.add_option("--scan2cfg",dest="scan2cfg",help="Convert scan.log into cfg file",
		action="store_true",default=False)
	copy_group.add_option("--unpack",dest="unpack",help="Run the full unpacking step",
		action="store_true",default=False)
	copy_group.add_option("--cfg2info",dest="cfg2info",action="store_true",default=False,
		help="Convert cfg to info dictionary")
	parser.add_option_group(copy_group)

	#SPM preprocessing and stats options
	spm_group = OptionGroup(parser,"SPM Preprocessing/1st Level Statistics")
	spm_group.add_option("--setup_preproc",dest="setup_preproc",action="store_true",
		default=False,help="Make spm (preproc) batch files")
	spm_group.add_option("--setup_stats",dest="setup_stats",
		help="Make spm (stats) batch files using rp_*.txt as mult regressors",action="store_true",
		default=False)
	spm_group.add_option("--setup_outliers",dest="setup_outliers",
		help="Make spm (stats) batch files using outliers as mult regressors",action="store_true",
		default=False)
	spm_group.add_option("--run_outliers",dest="run_outliers",help="Run outlier script",
		action="store_true",default=False)
	spm_group.add_option("--run_stats",dest="run_stats",help="Run stats script",action="store_true",
		default=False)
	spm_group.add_option("--run_preproc",dest="run_preproc",help="Run preproc script",
		action="store_true",default=False)
	spm_group.add_option("--makeMultCond",dest="makeMC",action="store_true",default=False,
		help="Parse exp logs for multiple condition information, must be done before --setup_[stats]")	
	spm_group.add_option("--run_art",dest="run_art",action="store_true",default=False,
		help="Run artifact detection for a subject")	
	parser.add_option_group(spm_group)
	
	#Cortical reconstruction options
	recon_group = OptionGroup(parser,"Cortical reconstructions with Freesurfer")
	recon_group.add_option("--setup_recon",dest="setup_recon",help="Setup cortical reconstruction",
		action="store_true",default=False)
	recon_group.add_option("--run_recon",dest="run_recon",help="Run reconstruction script",
		action="store_true",default=False)
	parser.add_option_group(recon_group)
	
	#MEG options
	meg_group = OptionGroup(parser,"MEG processing")
	meg_group.add_option("--preProc",dest="preProc",help="Run the preProc script for meg",
		action="store_true",default=False)
	meg_group.add_option("--preAnat",dest="preAnat",help="Run the preAnat script for meg",
		action="store_true",default=False)
	meg_group.add_option("--makeInv",dest="makeInv",action="store_true",default=False,
		help="Run the MEG makeInv script")
	meg_group.add_option("--run_ica",dest="run_ica",action="store_true",default=False,
		help="Run ICA processing (takes 90-120 minutes).")
	meg_group.add_option("--setup_bem",dest="setup_bem",help="Setup BEM folders/dicoms",
		action="store_true",default=False)
	meg_group.add_option("--new_preProc",dest="new_preProc",action="store_true",default=False,
		help="Run the new meg preprocessing.")	
	parser.add_option_group(meg_group)
	
	#Second-level options
	second_group = OptionGroup(parser,"Second-Level Statistics")
	second_group.add_option("--setup_second",dest="setup_second",action="store_true",default=False,
		help="Setup second level stats")
	second_group.add_option("--surf_second",dest="surf_second",action="store_true",default=False,
		help="Analyze second level stats (that have been run) with Freesurfer")
	second_group.add_option("--package_second",dest="package_second",action="store_true",default=False,
		help="Make analysis package (run after --surf_second)")
	second_group.add_option("--pvalue",dest="pvalue",type="float",default=0.001,
		help="Use with --surf_second")
	second_group.add_option("--date",dest="date",type="string",default=None,
		help="Specify a previously run second level date in the form of YYYYMMDD")
	second_group.add_option("--aparc",dest="aparc",action="store_true", default=False,
		help="If passed with --surf_second, tksurfer will load aparc annotations")
	second_group.add_option("--dry",dest="dry",action="store_true",default=False,
		help="If passed with --surf_second, all scripts are copied but tksurfer isn't run")
	second_group.add_option("--exc",dest="bad_sub",action="append",default=[],
		help="Exclude subjects, for use with --setup_second")
	second_group.add_option("--mask",dest="mask",action="append",default=[],
		help="At the second level, mask certain contrasts. Pass in <study>,<contrast>,<pathtomaskimage>")
	second_group.add_option("--inc",dest="good_sub",action="append",default=[],
		help="At the second level, include subjects, use with --setup_second")
	second_group.add_option("--run_second",dest="run_second",action="store_true",default=False,
		help="Run the script made my --setup_second")
	second_group.add_option("--override_p",dest="override_p",action="store_true",default=False,
		help="Overwrite p-value with new --pvalue")
	second_group.add_option("--all_second",dest="all_second",action="store_true",default=False,
		help="Run --setup_second,--run_second,--surf_second, and --package_second all in one call")
	parser.add_option_group(second_group)
	
	#Parallelization Options
	par_group = OptionGroup(parser,"Paralleization","WARNING:Use --joblib and --parallel together "
													"at your own risk")
	par_group.add_option("--ncores",dest="joblib",action="store",type="int",default=0,
		help="Process this many subjects in parallel")
	par_group.add_option("--parallel",dest="parallel",action="store_true",default=False,
		help="For use with --setup_*.Make the script so jobs run in parallel")
	par_group.add_option("--launchpad",dest="launchpad",action="store_true",default=False,
		help="Use with --run_*, pipeline only exits when all jobs finish")
	parser.add_option_group(par_group)
	
	#Miscellaneous options
	misc_group = OptionGroup(parser,"Miscellaneous")
	misc_group.add_option("--subject_list",dest="subject_list",action="store",default="",
		help="Run the pipeline for subjects in this file.")
	misc_group.add_option("--print_info",dest="print_info",action="store_true",default=False,
		help="Print subject's info.txt")
	misc_group.add_option("--verbose",dest="verbose",help="Print info",action="store_true",
		default=False)
	misc_group.add_option("--study",dest="single_study",help="Use with --setup_*",action="store",
		type="string",default=None)
	parser.add_option_group(misc_group)	


	(options,subjects) = parser.parse_args()
	data = dict({})
	#add options to data
	for opt,value in options.__dict__.items():
		data[opt] = value
	
	if data["subject_list"]:
		subjects = get_subjects(data["subject_list"])
	
	#transform parallel
	if data["parallel"]:
		data["parallel"] = "&"
	else:
		data["parallel"] = ""
	#transform aparc
	if data["aparc"]:
		data["aparc"] = "-aparc"
	else:
		data["aparc"] = ""
	#transform mask(s)
	if data["mask"]:
		masks = []
		for mask_str in data["mask"]:
			parts = mask_str.split(",")
			masks.append(parts)
		data["mask"] = masks
	data['stype'] = subject_type(subjects)
		
	#subject-level options
	if use_joblib and data["joblib"]:
		Parallel(n_jobs=data["joblib"],verbose=data["verbose"])(delayed(process_subject)(subject,data) for subject in subjects)
	else:
		[process_subject(subject,data) for subject in subjects]
	#group level options
	if data["all_second"]:
		second_level(data,"setup")
		second_level(data,"run")	
		second_level(data,"surf")
		second_level(data,"package")
	if data["setup_second"]:
		second_level(data,"setup")
	if data["run_second"]:
		second_level(data,"run")
	if data["surf_second"]:
		second_level(data,"surf")
	if data["package_second"]:
		second_level(data,"package")
	#miscellaneous
	if data["launchpad"]:
		pipeline.wait_to_finish(running_jobs)
