#!/usr/bin/env python

from optparse import OptionParser
import os
from subprocess import Popen, PIPE
from glob import glob
import getpass
import stat
import pipeline as pipeline
import sys
import time
import pickle

VER = "1.0"
possible_studies = dict({"ATLLoc":1,"MaskedMM":2,"BaleenMM":8,"AXCPT":2}) 
batch_dir = "/cluster/kuperberg/SemPrMM/MRI/scripts/spm_batches/"
func_dir = "/cluster/kuperberg/SemPrMM/MRI/functionals/"
dicom_dir = "/cluster/kuperberg/SemPrMM/MRI/dicoms/"
meg_dir = "/cluster/kuperberg/SemPrMM/MEG/data/"
meg_scripts = "/cluster/kuperberg/SemPrMM/MEG/scripts/"
mri_scripts = "/cluster/kuperberg/SemPrMM/MRI/scripts/"
running_jobs = [ ]


class ProgrammerError(Exception):
	pass


class UserError(Exception):
	pass


class SystemError(Exception):
	pass	


def archive_to_cluster(data):
	"""
	data: dict with keys:"subject","subject_dicom"
	Copies dicom images from archive dir to dicom dir.
	parallel subjects : YES
	"""
	if data["verbose"]:
		print("archive_to_cluster:")
	data["archive_dir"] = pipeline.find_session([data["subject"],"-x","Kuperberg"])
	if data["archive_dir"] is None:
		print("ALERT:findsession returned nothing for {0}".format(data["subject"]))
		return
	if not os.path.exists(data["dicom_dir"]):
		os.mkdir(data["dicom_dir"])
	pipeline.mirror(data["archive_dir"],data["dicom_dir"],True)


def scan_only(data):
	"""
	data: dict with keys:"subject_dir","subject_dicom"
	Runs unpacksdcmdir in -scanonly mode, which creates data["mri_dir"]/scan.log
	parallel subjects: NO
	"""
	if not os.path.exists(data["mri_dir"]):
		os.mkdir(data["mri_dir"])
	data["scan_path"] = scan_path(data)
	try:
		pipeline.scan_only(data["dicom_dir"],data["mri_dir"],data["scan_path"])
	except OSError:
		print("\nScan_only: Failed\n\nYou need to setup NMRENV before proceeding.\n\n")


def unpack(data):
	"""
	data: dict with keys:"subject_dir","subject_dicom","subject"
	Runs unpacksdcmdir in the full mode.
	Can take a long time, run with care.
	parallel subjects: NO
	"""
	data["cfg_path"] = cfg_path(data)
	if os.access(data["cfg_path"], os.R_OK):
		pipeline.unpack(data["dicom_dir"],data["mri_dir"],data["cfg_path"])
	else:
		print("ALERT: cannot find cfg file for {0}, re-run --scan2cfg".format(data["subject"]))


def cfg_path(data):
	"""
	data: dict with keys:"subject_dir"
	Returns path to cfg file.
	"""
	return os.path.join(data["mri_dir"],"cfg.txt")


def scan_path(data):
	"""
	data: dict with "subject_dir"
	returns path to scan.log file
	"""
	return os.path.join(data["mri_dir"],"scan.log")


def unpack_all(data):
	"""
	data: dict with keys:"subject_dir","subject_dicom","subject"
	Wrapper for all three pieces of unpacking a subject"s dicom dir.
	Runs:
		1) scan_only
		2) scan2cfg
		3) unpack
	parallel subjects: NO
	"""
	scan_only(data)
	scan2cfg(data)
	unpack(data)	


def scan2cfg(data):
	"""
	data: dict with keys:"subject_dir"
	Converts scan.log to appropriate cfg file and writes to disk.
	"""
	data["scan_path"] = scan_path(data)
	data["cfg_path"] = cfg_path(data)
	dir_map = dict({"MEMPRAGE_4e_p2_1mm_iso":"MPRAGE", "ge_functionals_atlloc":"ATLLoc", 
			"field_mapping":"FieldMap", "ge_functionals_maskmm":"MaskedMM", 
			"ge_functionals_baleen":"BaleenMM", "MEFLASH_8e_1mm_iso_5deg":"MEFLASH",
			"ge_functionals_axcpt":"AXCPT","ge_functionals_axcpt_sc":"AXCPT"})
	scan_list = [["MEMPRAGE_4e_p2_1mm_iso","ok","256","256","1"],
				["ge_functionals_atlloc","ok","160"],["ge_functionals_maskmm","ok","148"],
				["ge_functionals_baleen","ok","130"],["ge_functionals_axcpt","ok","240"],
				["MEFLASH_8e_1mm_iso_5deg","256","256","ok","1"],["field_mapping","ok","1"],
				["ge_functionals_axcpt_sc","ok","140"]]
	good_lines = pipeline.scan_to_cfg(dir_map,dir_map,scan_list,data["scan_path"])
	good_lines.sort(key=lambda x:int(x.split()[0]))
	fieldmaps = [v for v in good_lines if v.split()[1] == "FieldMap"]
	if len(fieldmaps) % 2 == 1:
		print("Odd amount of fieldmaps. --scan2cfg cannot continue. Copy/paste/edit as needed.")
		for line in good_lines:
			print(line)
		return
	good_fieldmaps = []
	func_names = ["MaskedMM","BaleenMM","ATLLoc","AXCPT"]
	for fm in fieldmaps:
		ind = good_lines.index(fm)
		all_prev_func = [r for r in good_lines[0:ind] if r.split()[1] in func_names]
		if len(all_prev_func) < 1:
			print("ALERT: odd field maps. Copy/paste/edit the dump below.")
			for line in good_lines:
				print(line)
			return
		prev_func = all_prev_func[len(all_prev_func)-1]
		run = fm.split()[0]
		func = prev_func.split()[1]
		parts = fm.split()
		(before,dot,filetype) = parts[3].partition(".")
		(nothing,maptext,num) = before.partition("FieldMap")
		if int(num)%2 == 1:
			postfix = "Mag"
		else:
			postfix = "Phase"
		good_fieldmaps.append("{0} {1} {2} FieldMap_{3}_{4}.{5}".format(parts[0],parts[1],parts[2],
			func,postfix,parts[2]))
	not_fieldmaps = [v for v in good_lines if v.split()[1] != "FieldMap"]
	for fm in good_fieldmaps:
		not_fieldmaps.append(fm)
	good_lines = not_fieldmaps
	good_lines.sort(key=lambda x:int(x.split()[0]))
	try:
		write_file_with_list(data["cfg_path"],"\n".join(good_lines),data["verbose"])
	except:
		print("Failure in scan2cfg")
		raise


def studies_to_setup(data,type):
	studies = []
	if "stats" in type:
		studies = find_proc_studies(data)
	elif "preproc" in type:
		studies = find_complete_studies(data)
	if data["single_study"]:
		studies = [data["single_study"]]
	return studies


def setup_spm(data,type):
	data["studies_to_setup"] = studies_to_setup(data,type)
	for study in data["studies_to_setup"]:
		job_dir = os.path.join(data["mri_dir"],study,"jobs")
		if not os.path.exists(job_dir):
			os.mkdir(job_dir)
		if "stats" in type:
			stat_dirs = []
			if study != "BaleenMM":
				stat_dirs.append(os.path.join(data["mri_dir"],study,type))
			else:
				for block in ["1","2"]:
					stat_dirs.append(os.path.join(data["mri_dir"],study,type+block))
			for smooth in ["6mm","8mm"]:
				for stat_dir in stat_dirs:
					if not os.path.exists(stat_dir):
						os.mkdir(stat_dir)
					smooth_dir = os.path.join(stat_dir,smooth)
					if not os.path.exists(smooth_dir):
						os.mkdir(smooth_dir)
		if "stats" in type and study == "BaleenMM":
			write_batch_files(data,study,type,"1")
			write_batch_files(data,study,type,"2")
		else:
			write_batch_files(data,study,type,"")
	write_shell_script(data,study,type)


def shell_script_path(data,study,type):
	script_dir = os.path.join(data["mri_dir"],"scripts")
	if not os.path.exists(script_dir):
		os.mkdir(script_dir)
	if "recon" in type:
		return os.path.join(script_dir,"recon"+"_"+data["subject"]+".sh")
	elif data["single_study"]:
		return os.path.join(script_dir,data["single_study"].lower()+"_"+type+"_"+data["subject"]+".sh")
	else:
		return os.path.join(script_dir,"all_"+type+"_"+data["subject"]+".sh")


def write_shell_script(data,study,type):
	shell_script = shell_script_path(data,study,type)
	commands = []
	commands.append("#!/bin/sh")
	if "recon" in type:
		setup_recon = "recon-all -s " + data["subject"]
		mprages = find_all_runs(data,"MPRAGE")
		for mprage in mprages:
			setup_recon = setup_recon + " -i " + mprage
		commands.append(setup_recon + " > {0}".format(os.path.join(data["mri_dir"],"scripts","setup_recon.log")))
		commands.append("nohup recon-all -all -s {0} -mail {1} >& /dev/null &".format(data["subject"],getpass.getuser()))
	else:
		if "stats" in type:
			commands.append("unset DISPLAY")
		mlab_cmd = "nohup matlab7.11 -nosplash -nodesktop"
		if "stats" in type:
			mlab_cmd += " -nodisplay "
		if not "studies_to_setup" in data:
			data["studies_to_setup"] = studies_to_setup(data,type)
		for study in data["studies_to_setup"]:
			if study == "BaleenMM" and "stats" in type:
				job_file = os.path.join(data["mri_dir"],study,"jobs",study.lower()+"_"+
					type+"_block1.m")
				log_file = os.path.join(data["mri_dir"],study,"jobs",study.lower()+"_"+
					type+"_block1.log")
				commands.append("{2} < {0} > {1} {3}".format(job_file,log_file,
					mlab_cmd,data["parallel"]))
				job_file = os.path.join(data["mri_dir"],study,"jobs",study.lower()+"_"+
					type+"_block2.m")
				log_file = os.path.join(data["mri_dir"],study,"jobs",study.lower()+"_"+
					type+"_block2.log")
				commands.append("{2} < {0} > {1} {3}".format(job_file,log_file,
					mlab_cmd,data["parallel"]))
			else:
				job_file = os.path.join(data["mri_dir"],study,"jobs",study.lower()+"_"+type+".m")
				log_file = os.path.join(data["mri_dir"],study,"jobs",study.lower()+"_"+type+".log")
				commands.append("{2} < {0}  > {1} {3}".format(job_file,log_file,
					mlab_cmd,data["parallel"]))
	try:
		write_file_with_list(shell_script,"\n".join(commands),data["verbose"])
	except IOError:
		print(commands)
		raise SystemError("Cannot write shell script. See text above traceback for shell commands.")
	make_file_exec(shell_script)		


def make_file_exec(path):
	os.chmod(path,stat.S_IRUSR|stat.S_IWUSR|stat.S_IXUSR|stat.S_IRGRP|stat.S_IWGRP|stat.S_IXGRP|stat.S_IROTH )
	
	
def specific_batch_path(data,study,type,block):
	if study == "BaleenMM" and "stats" in type:
		return os.path.join(data["mri_dir"],study,"jobs",study.lower()+"_"+
			type+"_block"+block+"_job.m")
	else:
		return os.path.join(data["mri_dir"],study,"jobs",study.lower()+"_"+type+"_job.m")
		

def write_batch_files(data,study,type,block):
	study_dict = build_funcdir_info(data,study,type,block)
	if "stats_outliers" in type:
		good_type = "stats"
	else:
		good_type = type
	if study == "BaleenMM" and "stats" in type:
		if block is None:
			raise ProgrammerError("write_batch_files: no block specified for baleenmm stats")
		path_to_batch = os.path.join(batch_dir,data["stype"],
			study.lower()+"_"+good_type+"_block"+block+"_job.m")
		out_batch_name = study.lower()+"_"+type+"_block"+block+"_job.m"
	else:		
		path_to_batch =os.path.join(batch_dir,data["stype"],study.lower()+"_"+good_type+"_job.m")
		out_batch_name = study.lower()+"_"+type+"_job.m"
	specific_batch_file = os.path.join(data["mri_dir"],study,"jobs",out_batch_name)
	pipeline.f2f_replace(path_to_batch,specific_batch_file,study_dict,data["verbose"])
	batch_dict = matlab_batch_dict(data,study,type,block)
	if study == "BaleenMM" and "stats" in type:
		if block is None:
			raise ProgrammerError("write_batch_files: no block specified for baleenmm stats")
		subject_batch = os.path.join(data["mri_dir"],study,"jobs",study.lower()+"_"+
			type+"_block"+block+".m")
	else:
		subject_batch = os.path.join(data["mri_dir"],study,"jobs",study.lower()+"_"+type+".m")
	pipeline.f2f_replace(os.path.join(batch_dir,data["stype"],"spm_mfile.m"),subject_batch,
		batch_dict,data["verbose"])


def matlab_batch_dict(data,study,type,block):
	replace_dict = dict({"[study]":study.lower(),"[type]":type,"[subject]":data["subject"]})
	if block:
		replace_dict["[block]"] = "_block"+block
	else:
		replace_dict["[block]"] = block
	if "stats" in type:
		replace_dict["[rm]"] = "1"
		replace_dict["[6mmDir]"] = os.path.join(data["mri_dir"],study,type+block,"6mm","*")
		replace_dict["[8mmDir]"] = os.path.join(data["mri_dir"],study,type+block,"8mm","*")
	else:
		replace_dict["[rm]"] = "0"
		replace_dict["[6mmDir]"] = ""
		replace_dict["[8mmDir]"] = ""
	replace_dict["[script]"] = specific_batch_path(data,study,type,block)
	replace_dict["[working_dir]"] = os.path.join(data["mri_dir"],study,"jobs")
	return replace_dict


def build_funcdir_info(data,study,type,block):
	"""build_funcdir_info:
	creates a dictionary that is used by write_batch_files to convert a generic batch file (e.g. 
	/cluster/kuperberg/SemPrMM/MRI/scripts/spm_batches/atlloc_stats_job.m to a file that actually 
	corresponds to a specific subject."""
	study_dict = dict();
	study_dict["[subject]"] = data["subject"]
	study_dict["[email_message]"] = " ".join([data["subject"],study,type,"finished."])
	study_dict["[stat_folder]"] = type
	#find paths for all the runs in the study/block
	all_runs = find_all_runs(data,study)
	for i,run in enumerate(all_runs):
		study_dict["[Run"+str(i+1)+"XXX]"] = os.path.basename(os.path.dirname(run))
	if "stats" in type:
		if study == "BaleenMM":
			study_dict["[email_message]"] = " ".join([data["subject"],study,type,"block "+block,
				"finished."])
			study_dict["[stat_folder]"] = type+block
			if block == "1":
				all_runs = find_all_runs(data,study,[1, 4])
				for i,run in enumerate(all_runs):
					study_dict["[Run"+str(i+1)+"XXX]"] = os.path.basename(os.path.dirname(run))				
					if "outliers" in type:
						study_dict["[Run"+str(i+1)+"MR]"] = ("art_regression_outliers_and_movement_"
							+study+str(i+1)+".mat")
					else:
						study_dict["[Run"+str(i+1)+"MR]"] = "rp_"+study+str(i+1)+".txt"
			elif block == "2":
				all_runs = find_all_runs(data,study,[5, 8])
				for i,run in enumerate(all_runs):
					study_dict["[Run"+str(i+5)+"XXX]"] = os.path.basename(os.path.dirname(run))
					if "outliers" in type:
						study_dict["[Run"+str(i+5)+"MR]"] = ("art_regression_outliers_and_movement_"
							+study+str(i+5)+".mat")
					else:
						study_dict["[Run"+str(i+5)+"MR]"] = "rp_"+study+str(i+5)+".txt"
			else:
				raise ProgrammerError("Build_func_dir: baleen stats not specificied with block")
		else: #not baleen
			all_runs = find_all_runs(data,study)
			for i,run in enumerate(all_runs):
				study_dict["[Run"+str(i+1)+"XXX]"] = os.path.basename(os.path.dirname(run))
				if "outliers" in type:
					study_dict["[Run"+str(i+1)+"MR]"] = ("art_regression_outliers_and_movement_"+
						study+str(i+1)+".mat")
				else:
					study_dict["[Run"+str(i+1)+"MR]"] = "rp_"+study+str(i+1)+".txt"
	elif type == "preproc":
		#find mprage xxx
		mprage = glob(os.path.join(data["mri_dir"],"MPRAGE","*","MPRAGE*.nii"))
		if len(mprage) == 0:
			raise UserError("Error: cannot find any mprage scans for this subject. Fix immediately")
		study_dict["[MPRAGEXXX]"] = os.path.basename(os.path.dirname(mprage[0]))
		#find phase fieldmap xxx
		fieldmap_phase = glob(os.path.join(data["mri_dir"],"FieldMap","*","*"+study+"_Phase.nii"))
		if len(fieldmap_phase) == 0:
			raise UserError("Error:cannot find fieldmap for "+study+" phase. Fix immediately") 
		study_dict["[FieldMapPhaseXXX]"] = os.path.basename(os.path.dirname(fieldmap_phase[0]))
		fieldmap_mag = glob(os.path.join(data["mri_dir"],"FieldMap","*","*"+study+"_Mag.nii"))
		if len(fieldmap_phase) == 0:
			raise UserError("Error: cannot find fieldmap for "+study+" mag. Fix immediately")
		study_dict["[FieldMapMagXXX]"] = os.path.basename(os.path.dirname(fieldmap_mag[0]))
	return study_dict	


def find_all_runs(data,study,fl="all"):
	"""find_all_runs:
	Searches data["mri_dir"]/study for desired runs. Normally, fl is "all", so all runs of a
	study are returned.  fl can be a 2-item list in which the first item is the lower bound run # 
	and the 2nd item is the upper bound item run number.  This is useful for BaleenMM, in which
	sometimes we only want the first four or second four runs."""
	if fl == "all":
		number_str = "*"
	else:
		number_str = "["+str(fl[0])+"-"+str(fl[1])+"]"
	return glob(os.path.join(data["mri_dir"],study,"*",study+number_str+".nii"))


def find_studies_run(data):
	studies_run = []
	for study in possible_studies:
		if os.path.exists(os.path.join(data["mri_dir"],study)):
			studies_run.append(study)
			all_runs = find_all_runs(data,study)
			if (len(all_runs) == possible_studies[study]) or (
			len(all_runs) == 3  and study == "AXCPT" and data["stype"] == "ac"):
				touch_file = os.path.join(data["mri_dir"],study,"complete")
			else:
				touch_file = os.path.join(data["mri_dir"],study,"incomplete")
			open(touch_file,"w").close();
	return studies_run	


def find_proc_studies(data):
	"""find_proc_studies:
	Searches for data["mri_dir"]/[study]/jobs/[study]_preproc_run.  This file is made after a 
	preprocessing batch completes.  Since preprocessing takes the most time (and thus, want to skip 
	if it"s already run), this doesn"t look for *_stats_run files (since stats jobs takes at most 5 
	minutes)"""
	run_studies = []
	possible = possible_studies.keys()
	for study in possible:
		run_file = os.path.join(data["mri_dir"],study,"jobs",study.lower()+"_preproc_run")
		if data["verbose"]:
			print("Found a run file at {0}".format(run_file))
		if os.path.exists(run_file):
			run_studies.append(study)
	return run_studies


def find_complete_studies(data):
	complete_studies = []
	data["studies_run"] = find_studies_run(data)
	for study in data["studies_run"]:
		complete_file = os.path.join(data["mri_dir"],study,"complete")
		nosetup_file = os.path.join(data["mri_dir"],study,"NO_SETUP")
		if os.path.exists(complete_file) and not os.path.exists(nosetup_file):
			complete_studies.append(study)
	return complete_studies


def setup_recon(data):
	write_shell_script(data,"","recon")


def run_mri_script(data,type):
	#find all the scripts of this type
	if data["single_study"]:
		prefix = data["single_study"].lower()+"_"
	else:
		prefix = "all_"
	scripts = glob(os.path.join(data["mri_dir"],"scripts",prefix+type+"_*"))
	for script in scripts:
		args = ["nohup", script]
		print("\nStarting {0}\n".format(script))
		process = pipeline.run_process(args)
		running_jobs.append(process)


def meg_script(data,type,extra=None):
	"""
	Run a particular meg script as given in type.
	data: the regular dict
	type: name of meg script (e.g. "preProc1","preAnat1",etc)
	extra: list of extra arguments (e.g. for "preProc2", extra would be [{preBlinkTime},
	{postBlinkTime}])
	"""
	script_path = os.path.join(meg_scripts,type)
	if not os.path.exists(script_path):
		raise UserError("meg_script: can not locate {0}".format(type))
	my_args = [script_path,data["subject"]]
	if type == "preProc":
		#look for fif
		fif = glob(os.path.join(data["meg_dir"],"*_raw.fif"))
		#filter our blink and emptyroom
		fif = [x for x in fif if x.find("Blink") == -1 and x.find("emtpyroom") == -1]
		N_fif = len(fif)
		N_icamat = len(glob(os.path.join(data["meg_dir"],"*_raw_ica.mat")))
		N_blinks = len(glob(os.path.join(data["meg_dir"],"*.blinks")))
		if (N_fif == 0):
			raise UserError("No fif files found for this subject")
		if not (N_fif == N_icamat == N_blinks):
			raise UserError("Unequal amount of fif,ica.mat,and blink files.\nTry Again")
		# DEFAULT EYE BLINK ARGUMENTS FOR PREPROC1
		extra = ["0.1","0.3"]
	if type == "preAnat":
		setup_bem(data)
		#check that flash was unpacked
		if os.path.exists(os.path.join(data["mri_dir"],"MEFLASH")):
			extra = ["FLASH"]
		else:
			extra = ["WATER"]
	if extra:
		my_args.extend(extra)
	#start the process 
	log_file = "{0}{1}_{2}.log".format(data["meg_dir"],data["subject"],type)
	print("Logging to {0}".format(log_file))
	f = open(log_file,"w")
	process = pipeline.run_process(my_args,output=f,error=f)
	if data["parallel"]:
		running_jobs.append(process)	
	else:
		process.communicate()[0]


def setup_bem(data):
	print("Setup_BEM:{0}".format(data["subject"]))
	#must have env var set
	if os.getenv("SUBJECTS_DIR") is None:
		raise UserError("$SUBJECTS_DIR is not set")
	recon_dir = os.path.join(os.getenv("SUBJECTS_DIR"),data["subject"])
	#if the recon hasn"t run yet quit
	if not os.path.exists(recon_dir):
		raise UserError("Reconstruction folders not setup. Start recon and try again later.")
	bem_dir = os.path.join(recon_dir,"bem")
	if not os.path.exists(bem_dir):
		os.mkdir(os.path.join(recon_dir,"bem"))
	flash_dcm_dir = os.path.join(bem_dir,"flash_dcm")
	if not os.path.exists(flash_dcm_dir):	
		os.mkdir(flash_dcm_dir)
	flash_org_dir = os.path.join(bem_dir,"flash_org")	
	if not os.path.exists(flash_org_dir):	
		os.mkdir(flash_org_dir)
	try:
		scanlog_path = scan_path(data)
		all_lines = list_from_file(scanlog_path,data["verbose"])
		dcm = [line.split()[7] for line in all_lines if "MEFLASH_8e_1mm_iso_5deg" in line and " 8 " in line and " err " not in line]
		if len(dcm) > 1:
			ProgrammerError("More than one MEFLASH5 with 8 frames...check scan.log")
		elif len(dcm) == 0:
			dcm = None;		
		else:
			dcm = dcm[0]
	except IOError:
		raise ProgrammerError("Could not open scan.log file")
	if dcm:
		(sub_dcm,dash,after) = dcm.rpartition("-")
		search_dir = os.path.join(data["dicom_dir"],sub_dcm+"*.dcm")
		flash_dcms = glob(search_dir)
		flash_dcms.sort()
		for flash_dcm in flash_dcms:
			(path,slash,filename) = flash_dcm.rpartition("/")
			try:
				sympath = os.path.join(flash_dcm_dir,filename)
				os.symlink(flash_dcm,sympath)
			except OSError:
				pass
	else:
		print("NO Flash DCM...")


def set_paths(data):
	"""Fairly self-explanatory.  Adds keys to the dictionary for "mri_dir","subject_dir",
	"dicom_dir",and "subject_dicom".  path prefixes are hard coded.
	"""
	data["mri_dir"] = os.path.join(func_dir,data["subject"])+os.sep
	data["dicom_dir"] = os.path.join(dicom_dir,data["subject"])+os.sep
	data["meg_dir"] = os.path.join(meg_dir,data["subject"])+os.sep


def second_level(data,type):
	prefix = os.path.join(func_dir,"SecondLevelStats")
	study_contrasts = dict({"ATLLoc":[["Nonwords","0003"],["WordLists","0002"],["Sentences","0001"],
		["SentencesVWordLists","0004"],["SentencesVNonwords","0006"],["WordListsVNonwords","0008"]],
		"MaskedMM":[["Dir","0001"],["UnRel","0003"],["UnRelVDir","0007"]],
		"BaleenLP":[["Rel","0001"],["UnRel","0002"],["UnRelVRel","0007"]],
		"BaleenHP":[["Rel","0001"],["UnRel","0002"],["UnRelVRel","0008"],
		["UnRelFillvRelFill","0009"]],"AXCPT":[["AYvBY","0009"],["BXvBY","0011"]]})
	study_stats = dict({"ATLLoc":"stats_outliers","MaskedMM":"stats_outliers",
		"BaleenLP":"stats_outliers1","BaleenHP":"stats_outliers2","AXCPT":"stats_outliers"})
	if data["date"]:
		date_dir = data["date"]
	else:
		date_dir = time.strftime("%Y%m%d")
	
	if type == "setup":
		second_setup(data,prefix,date_dir,study_contrasts,study_stats)
	if type == "surf":
		second_surf(data,prefix,date_dir,study_contrasts,study_stats)
	if type == "package":
		second_package(data,prefix,date_dir,study_contrasts,study_stats)

		
def second_package(data,prefix,date,study_contrasts,study_stats):
	import shutil
	print("Packaging second level surf analysis....\n")
	package_dir = os.path.join(prefix,"surf_analysis_"+date)
	surf_dir = os.path.join(mri_scripts,"surf_analysis")
	if not os.path.exists(package_dir):
		os.mkdir(package_dir)
	if data["single_study"]:
		studies = [data["single_study"]]
	else:
		studies = ["ATLLoc","BaleenLP","BaleenHP","MaskedMM","AXCPT"]
	#copy index.html
	shutil.copyfile(os.path.join(surf_dir,"index.html"),
		os.path.join(package_dir,"index.html"))
	for study in studies:
		date_dir = os.path.join(prefix,data["stype"],study,date)
		#copy <study>.html
		shutil.copyfile(os.path.join(surf_dir,study+".html"),
			os.path.join(package_dir,study+".html"))
		#within package_dir, make study/
		study_dir = os.path.join(package_dir,study)
		if not os.path.exists(study_dir):
			os.mkdir(study_dir)
		for [contrast,XXXX] in study_contrasts[study]:
			print("{0}:\t{1}...".format(study,contrast))
			#within study_dir, make con/
			res_con_dir = os.path.join(study_dir,contrast+"_img")
			if not os.path.exists(res_con_dir):
				os.mkdir(res_con_dir)
			#data_con_dir points to data contrast directory
			img_con_dir = os.path.join(date_dir,contrast,"img")
			#within every data_con_dir is which holds all jpgs
			#we want to copy these jpgs to res_con_dir
			print("Copying images...")
			jpgs = glob(os.path.join(img_con_dir,"*.jpg"))
			for jpg in jpgs:
				shutil.copy(jpg,res_con_dir)
			#let's find N
			if "Baleen" in study:
				real_study = "BaleenMM"
			else:
				real_study = study
			search_term = os.path.join(func_dir,"*",real_study,study_stats[study],"8mm",
				"con_"+XXXX+".img")
			N = len(glob(search_term))
			#now, make the <con>.html file
			replace_dict = dict({"[N]":str(N),"[pvalue]":str(data["pvalue"]),
				"[contrast]":contrast,"[study]":study,"[stype]":data["stype"]})
			pipeline.f2f_replace(os.path.join(surf_dir,"contrast.html"),
				os.path.join(study_dir,contrast+".html"),replace_dict,data["verbose"])

			
def second_surf(data,prefix,date,study_contrasts,study_stats):
	from scipy import stats
	import shutil
	print("We're going surfing....\n")
	if data["single_study"]:
		studies = [data["single_study"]]
	else:
		studies = ["ATLLoc","BaleenLP","BaleenHP","MaskedMM","AXCPT"]
	for study in studies:
		date_dir = os.path.join(prefix,data["stype"],study,date)
		for [contrast,XXXX] in study_contrasts[study]:
			print("{0}:\t{1}...".format(study,contrast))
			con_dir = os.path.join(date_dir,contrast)
			#first, check that --setup_second has been run by looking for con_dir
			if not os.path.exists(con_dir):
				print("ERROR:--setup_second hasn't been run.")
				print("Do this and try again or pass in a specific date by --date=YYYYMMDD")
				break
			#next, check that SPM has been run and that spmT_0001.img exists
			t_img_exists = len(glob(os.path.join(con_dir,"spmT_0001.img"))) == 1
			if not t_img_exists:
				print("ERROR:SPM hasn't been run on {0}:{1}".format(study,contrast))
				break
			#spmT_0001.img exists, let's proceed
			#let's find N
			if "Baleen" in study:
				real_study = "BaleenMM"
			else:
				real_study = study
			search_term = os.path.join(func_dir,"*",real_study,study_stats[study],"8mm",
				"con_"+XXXX+".img")
			N = len(glob(search_term))
			script_dict = dict({})
			tvalue = stats.t.isf(data["pvalue"],N)
			print("T-threshold at N={0} and p<{1} is {2}".format(N,data["pvalue"],tvalue))
			script_dict["[tvalue]"] = str(tvalue)
			script_dict["[aparc]"] = data["aparc"]
			new_script_path = os.path.join(con_dir,"make_images.sh")
			pipeline.f2f_replace(os.path.join(mri_scripts,"surf_analysis","make_images.sh"),
				new_script_path,script_dict,data["verbose"])
			make_file_exec(new_script_path)
			#copy lh_tiffs and rh_tiffs
			for surf_script in ["lh_tiff","rh_tiff"]:
				tiffs_path = os.path.join(mri_scripts,"surf_analysis",surf_script)
				new_tiffs_path = os.path.join(con_dir,surf_script)
				shutil.copyfile(tiffs_path,new_tiffs_path)
				
			if not data["dry"]:
				# all ready to run make_images.sh
				os.chdir(con_dir)
				try:
					process = pipeline.run_process([new_script_path],sys.stdout)
					output = process.communicate()[0]
				except KeyboardInterrupt:
					sys.exit("Surf canceled by user...Goodbye")

			
def second_setup(data,prefix,date_dir,study_contrasts,study_stats):
	mlab_cmd = "matlab7.11 -nodisplay -nosplash -nodesktop "
	shell_commands = []
	shell_commands.append("#!/bin/sh")
	shell_commands.append("unset DISPLAY")
	if data["single_study"]:
		studies = [data["single_study"]]
	else:
		studies = ["ATLLoc","BaleenLP","BaleenHP","MaskedMM","AXCPT"]
	for study in studies:
		new_dir = os.path.join(prefix,data['stype'],study,date_dir)
		#make the new dir
		if not os.path.exists(new_dir):
			os.mkdir(new_dir)
		job_dir = os.path.join(new_dir,"jobs")
		if not os.path.exists(job_dir):
			os.mkdir(job_dir)
		print("{0}:\t{1} contrasts".format(study,len(study_contrasts[study])))
		for [contrast,XXXX] in study_contrasts[study]:
			replace_dict = dict()
			replace_dict["[contrast]"] = contrast
			con_dir = os.path.join(new_dir,contrast)
			replace_dict["[con_dir]"] = con_dir
			if not os.path.exists(con_dir):
				os.mkdir(con_dir)
			if "Baleen" in study:
				real_study = "BaleenMM"
			else:
				real_study = study
			search_term = os.path.join(func_dir,"*",real_study,study_stats[study],"8mm",
				"con_"+XXXX+".img")
			all_img = glob(search_term)
			good_img = [x for x in all_img if len([bad for bad in data["bad_sub"] if bad in x]) == 0]
			good_img[:] = ["'"+x+"'" for x in good_img]
			N = len(good_img)
			replace_dict["[contrast_images]"] = "\n".join(good_img)
			#for now
			replace_dict["[mask]"] = ""
			ibatch = os.path.join(batch_dir,data["stype"],"generic_2nd_batch.m")
			obatch = os.path.join(job_dir,contrast+"_job.m")
			iscript = os.path.join(batch_dir,data["stype"],"spm_2nd_mfile.m")
			oscript = os.path.join(job_dir,contrast+".m")
			#make batch
			pipeline.f2f_replace(ibatch,obatch,replace_dict)
			replace_dict["[study]"] = study
			replace_dict["[batch]"] = obatch
			#make script
			pipeline.f2f_replace(iscript,oscript,replace_dict)
			shell_commands.append("rm {0}/SPM.mat".format(con_dir))
			log = os.path.join(job_dir,contrast+".log")
			shell_commands.append("{0} < {1} > {2} {3}".format(mlab_cmd,oscript,log,data["parallel"]))
			info_fname = os.path.join(con_dir,"info.txt")
			dat_to_save = dict({"N":N,"images":good_img,"stype":data["stype"],"contrast":contrast})
			with open(info_fname,"w") as f:
				pickle.dump(dat_to_save,f)				
	shell_path = os.path.join(func_dir,"SecondLevelStats",data["stype"],"all_studies.sh")
	write_file_with_list(shell_path,"\n".join(shell_commands),data["verbose"])
	make_file_exec(shell_path)

		
def subject_type(subjects):
	# assume first is good
	if len(subjects) == 1:
		stype = subjects[0][:2]
	else:
		stype = subjects[0][:2]
		match = len([x for x in subjects[1:] if x[:2] == stype]) == (len(subjects) - 1)
		if not match:
			print("\nWarning, you seem to be processing subjects of differing types!!!!\n")
			response = 'n'
			response = input("Do you wish to continue? (y/n) : ")
			if response != "y":
				raise UserError("Try again with a better subject list")
	return stype


def write_file_with_list(path,lines,verbose=None):
	try:
		with open(path,'w') as f:
			f.writelines(lines)
		if verbose:
			print("Wrote {0}".format(path))
	except IOError:
		raise


def list_from_file(path,verbose=None):
	try: 
		with open(path,'r') as f:
			all_lines = f.read()
			all_lines = all_lines.splitlines()
			if verbose:
				print("Reading from {0}".format(path))
	except IOError:
		print("Cannot open {0}".path)
		raise
	return all_lines


def main():
	parser = OptionParser(usage="%prog [options] subject (more subjects)", version="%prog 1.0")
	parser.add_option("-d","--copy_dicom",dest="copy_dicom", 
		help="Copy from archive to $DICOM_DIR/[subject]",action="store_true",default=False)
	parser.add_option("-u","--unpack_all",dest="unpack_all",action="store_true",default=False,
		help="If specified, unpacking occurs in $FUNCTIONAL_DIR/[subject]")
	parser.add_option("-v","--verbose",dest="verbose",help="Print info",action="store_true",
		default=False)
	parser.add_option("-p","--setup_preproc",dest="setup_preproc",action="store_true",
		default=False,help="Make spm (preproc) batch files")
	parser.add_option("-r","--setup_recon",dest="setup_recon",help="Setup cortical reconstruction",
		action="store_true",default=False)
	parser.add_option("-s","--setup_stats",dest="setup_stats",
		help="Make spm (stats) batch files using rp_*.txt as mult regressors",action="store_true",
		default=False)
	parser.add_option("-o","--setup_outliers",dest="setup_outliers",
		help="Make spm (stats) batch files using outliers as mult regressors",action="store_true",
		default=False)
	parser.add_option("--study",dest="single_study",help="Setup single study",action="store",
		type="string",default=None)
	parser.add_option("--run_outliers",dest="run_outliers",help="Run outlier script",
		action="store_true",default=False)
	parser.add_option("--run_stats",dest="run_stats",help="Run stats script",action="store_true",
		default=False)
	parser.add_option("--run_preproc",dest="run_preproc",help="Run preproc script",
		action="store_true",default=False)
	parser.add_option("--run_recon",dest="run_recon",help="Run reconstruction script",
		action="store_true",default=False)
	parser.add_option("--setup_bem",dest="setup_bem",help="Setup BEM folders/dicoms",
		action="store_true",default=False)
	parser.add_option("--scan_only",dest="scan_only",help="Run only the scan_only unpack",
		action="store_true",default=False)
	parser.add_option("--scan2cfg",dest="scan2cfg",help="Convert scan.log into cfg file",
		action="store_true",default=False)
	parser.add_option("--unpack",dest="unpack",help="Run the full unpacking step",
		action="store_true",default=False)
	parser.add_option("--parallel",dest="parallel",help="Make the script so jobs run in parallel",
		action="store_true",default=False)
	parser.add_option("--preProc",dest="preProc",help="Run the preProc script for meg",
		action="store_true",default=False)
	parser.add_option("--preAnat",dest="preAnat",help="Run the preAnat script for meg",
		action="store_true",default=False)
	parser.add_option("--launchpad",dest="launchpad",action="store_true",default=False,
		help="Use with --run_*, pipeline only exits when all jobs finish")
	parser.add_option("--setup_second",dest="setup_second",action="store_true",default=False,
		help="Setup second level stats")
	parser.add_option("--surf_second",dest="surf_second",action="store_true",default=False,
		help="Analyze second level stats (that have been run) with Freesurfer")
	parser.add_option("--package_second",dest="package_second",action="store_true",default=False,
		help="Make analysis package (run after --surf_second)")
	parser.add_option("--pvalue",dest="pvalue",type="float",default=0.001,
		help="Use with --surf_second")
	parser.add_option("--date",dest="date",type="string",default=None,
		help="Specify a previously run second level date in the form of YYYYMMDD")
	parser.add_option("--aparc",dest="aparc",action="store_true", default=False,
		help="If passed with --surf_second, tksurfer will load aparc annotations")
	parser.add_option("--dry",dest="dry",action="store_true",default=False,
		help="If passed with --surf_second, all scripts are copied but tksurfer isn't run")
	parser.add_option("--makeInv",dest="makeInv",action="store_true",default=False,
		help="Run the MEG makeInv script")
	parser.add_option("--bad",dest="bad_sub",action="append",default=[],
		help="Exclude subjects, for use with --setup_second")
		
		
	(options,args) = parser.parse_args()
	data = dict({})
	#add options to data
	for opt,value in options.__dict__.items():
		data[opt] = value
	
	#transform parallel
	if data["parallel"]:
		data["parallel"] = "&"
	else:
		data["parallel"] = ""
	#transform aparc
	if data["aparc"]:
		data["aparc"] = "-aparc"
	else:
		data["aparc"] = ""
	data['stype'] = subject_type(args)
	for subject in args:
		try:
			data["subject"] = subject
			set_paths(data)	
			if data["copy_dicom"]:		
				archive_to_cluster(data)
			if data["scan_only"]:
				scan_only(data)
			if data["scan2cfg"]:
				scan2cfg(data)
			if data["unpack"]:
				unpack(data)
			if data["unpack_all"]:
				unpack_all(data)
			if data["setup_preproc"]:
				setup_spm(data,"preproc")
			if data["setup_stats"]:
				setup_spm(data,"stats")
			if data["setup_outliers"]:
				setup_spm(data,"stats_outliers")
			if data["setup_recon"]:
				setup_recon(data)
			if data["run_preproc"]:
				run_mri_script(data,"preproc")
			if data["run_stats"]:
				run_mri_script(data,"stats")
			if data["run_outliers"]:
				run_mri_script(data,"stats_outliers")
			if data["run_recon"]:
				run_mri_script(data,"recon")
			if data["setup_bem"]:
				setup_bem(data)
			if data["preProc"]:
				meg_script(data,"preProc")
			if data["preAnat"]:
				meg_script(data,"preAnat")
			if data["makeInv"]:
				meg_script(data,"makeInv")
		except (ProgrammerError,UserError, SystemError) as (errno,strerror):
			print(strerror)
			print("Something happened for {0}".format(data["subject"]))
	if data["setup_second"]:
		second_level(data,"setup")
	if data["surf_second"]:
		second_level(data,"surf")
	if data["package_second"]:
		second_level(data,"package")
	if data["launchpad"]:
		pipeline.wait_to_finish(running_jobs)

if __name__ == "__main__":
	main()
